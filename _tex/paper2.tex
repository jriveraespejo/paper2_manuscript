% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  authoryear,
  review,
  1p]{elsarticle}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{Psychometrika}

\usepackage[]{natbib}
\bibliographystyle{elsarticle-harv}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Let's talk about Thurstone \& Co.: An information-theoretical model for comparative judgments, and its statistical translation},
  pdfauthor={Jose Manuel Rivera Espejo; Tine van van Daal; Sven De De Maeyer; Steven Gillis},
  pdfkeywords={causal inference, directed acyclic graphs, structural
causal models, bayesian statistical methods, thurstonian
model, comparative judgement, probability, statistical modeling},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{Let's talk about Thurstone \& Co.: An information-theoretical
model for comparative judgments, and its statistical translation}
\author[1]{Jose Manuel Rivera Espejo%
\corref{cor1}%
}
 \ead{JoseManuel.RiveraEspejo@uantwerpen.be} 
\author[1]{Tine van Daal%
%
}
 \ead{tine.vandaal@uantwerpen.be} 
\author[1]{Sven De Maeyer%
%
}
 \ead{sven.demaeyer@uantwerpen.be} 
\author[2]{Steven Gillis%
%
}
 \ead{steven.gillis@uantwerpen.be} 

\affiliation[1]{organization={University of Antwerp, Training and
education sciences},,postcodesep={}}
\affiliation[2]{organization={University of
Antwerp, Linguistics},,postcodesep={}}

\cortext[cor1]{Corresponding author}




        
\begin{abstract}
This study revisits Thurstone's law of comparative judgments (CJ) by
addressing two key limitations in traditional approaches. Firstly, it
addresses the overreliance on the assumptions of Thurstone's Case V in
the statistical analysis of CJ data. Secondly, it addresses the apparent
disconnect between CJ's approach to trait measurement and hypothesis
testing. We put forward a systematic approach based on causal analysis
and Bayesian statistical methods, which results in a model that
facilitates a more comprehensive understanding of the factors
influencing CJ experiments while offering a robust statistical
translation. The new model accommodates unequal dispersions and
correlations between stimuli, enhancing the reliability and validity of
CJ's trait estimation, thereby ensuring the accurate measurement and
interpretation of comparative data. The paper highlights the relevance
of this updated framework for modern empirical research, particularly in
education and social sciences. This contribution advances current
research methodologies, providing a robust foundation for future
applications in diverse fields.
\end{abstract}





\begin{keyword}
    causal inference \sep directed acyclic graphs \sep structural causal
models \sep bayesian statistical methods \sep thurstonian
model \sep comparative judgement \sep probability \sep 
    statistical modeling
\end{keyword}
\end{frontmatter}
    

\newcommand{\dsep}{\:\bot\:}
\newcommand{\ndsep}{\:\not\bot\:}
\newcommand{\cond}{\:|\:}

\section{Introduction}\label{sec-introduction}

In \emph{comparative judgment} (CJ) studies, judges assess a specific
trait or attribute across different stimuli by performing pairwise
comparisons \citep{Thurstone_1927a, Thurstone_1927b}. Each comparison
produces a dichotomous outcome, indicating which stimulus is perceived
to have a higher trait level. For example, when assessing writing
quality, judges compare pairs of written texts (the stimuli) to
determine the relative writing quality each text exhibit (the trait)
\citep{Laming_2004, Pollitt_2012b, Whitehouse_2012, vanDaal_et_al_2016, Lesterhuis_2018_thesis, Coertjens_et_al_2017, Goossens_et_al_2018, Bouwer_et_al_2023}.

Numerous studies have documented the effectiveness of CJ in assessing
traits and competencies over the past decade. These studies have
highlighted three aspects of the method's effectiveness: its
reliability, validity, and practical applicability. Research on
reliability suggests that CJ requires a relatively modest number of
pairwise comparisons \citep{Verhavert_et_al_2019, Crompvoets_et_al_2022}
to generate trait scores that are as precise and consistent as those
generated by other assessment methods
\citep{Coertjens_et_al_2017, Goossens_et_al_2018, Bouwer_et_al_2023}. In
addition, the evidence suggests that the reliability and time efficiency
of CJ are comparable, if not superior, to those of other assessment
methods when employing adaptive comparison algorithms
\citep{Pollitt_2012b, Verhavert_et_al_2022, Mikhailiuk_et_al_2021}.
Meanwhile, research on the validity of CJ scores indicates their
capacity to represent accurately the traits under measurement
\citep{Whitehouse_2012, vanDaal_et_al_2016, Lesterhuis_2018_thesis, Bartholomew_et_al_2018, Bouwer_et_al_2023}.
Moreover, research on CJ's practical applicability highlights its
versatility across both educational and non-educational contexts
\citep{Kimbell_2012, Jones_et_al_2015, Bartholomew_et_al_2018, Jones_et_al_2019, Marshall_et_al_2020, Bartholomew_et_al_2020, Boonen_et_al_2020}.

Nevertheless, despite the increasing number of CJ studies, research in
this domain remains unsystematic and fragmented, leaving several
critical issues unresolved. This study identifies and discusses two
prominent issues that can undermine the reliability and validity of CJ's
trait estimates. The first issue arises from the excessive reliance on
Thurstone's Case V assumptions in the statistical analysis of CJ data.
The second stems from the apparent disconnect between CJ's approach to
trait measurement and hypothesis testing. The study then addresses these
issues by extending Thurstone's general form through a systematic and
integrated approach based on causal and Bayesian inference methods.

As a result, the study divides its content into six main sections.
Section~\ref{sec-thurstone_theory} provides an overview of Thurstone's
theory. Section~\ref{sec-theory-issues} discusses the identified issues
in detail. Section~\ref{sec-theoretical} extends Thurstone's general
form to address these challenges. The new model integrates the
theoretical core principles alongside key assessment design features
relevant to CJ experiments, such as the selection of judges, stimuli,
and comparisons. Section~\ref{sec-statistical} translates these
theoretical and practical elements into a probabilistic statistical
model for the analysis of pairwise comparison data.
Section~\ref{sec-discussion} discusses the implications of the findings
and explores avenues for future research. Finally,
Section~\ref{sec-conclusion} summarizes the study's key insights.

\section{Thurstone's theory}\label{sec-thurstone_theory}

In its most general form, Thurstone's theory addresses pairwise
comparisons wherein a single judge evaluates multiple stimuli
\citep[pp.~267]{Thurstone_1927b}. The theory posits that two key factors
determine the dichotomous outcome of these comparisons: the discriminal
process of each stimulus and their discriminal difference. The
\emph{discriminal process} captures the psychological impact each
stimulus exerts on the judge or, more simply, his perception of the
stimulus trait. The theory assumes that the discriminal process for any
given stimulus forms a Normal distribution along the trait continuum
\citep[pp.~266]{Thurstone_1927b}. The mode (mean) of this distribution,
known as the \emph{modal discriminal process}, indicates the stimulus
position on this continuum, while its dispersion, referred to as the
\emph{discriminal dispersion}, reflects variability in the perceived
trait of the stimulus.

Figure~\ref{fig-discriminal_process} illustrates the hypothetical
discriminal processes along a quality trait continuum for two written
texts. The figure indicates that the modal discriminal process for Text
B is positioned further along the continuum than that of Text A
\((T_{B} > T_{A})\), suggesting that Text B exhibits higher quality.
Additionally, the figure highlights that Text B has a broader
distribution compared to Text A, which arises from its larger
discriminal dispersion \((\sigma_{B} > \sigma_{A})\).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/discriminal_process.png}

}

\subcaption{\label{fig-discriminal_process}Discriminal processes}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/discriminal_difference.png}

}

\subcaption{\label{fig-discriminal_difference}Discriminal difference}

\end{minipage}%

\caption{\label{fig-thurstone_theory}Hypothetical discriminal processes
and discriminant difference along a quality trait continuum for two
written texts.}

\end{figure}%

However, since the individual discriminal processes of the stimuli are
not directly observable, the theory introduces the \emph{law of
comparative judgment}. This law posits that in pairwise comparisons, a
judge perceives the stimulus with a discriminal process positioned
further along the trait continuum as possessing more of the trait
\citep[pp.~251]{Bramley_2008}. This suggests that the relative distance
between stimuli, rather than their absolute positions on the continuum,
likely defines the outcome of pairwise comparisons. Indeed, the theory
assumes that the difference between the underlying discriminal processes
of the stimuli, referred to as the \emph{discriminal difference},
determines the observed dichotomous outcome. Furthermore, the theory
assumes that because the individual discriminal processes form a Normal
distribution on the continuum, the discriminal difference will also
conform to a Normal distribution \citep{Andrich_1978}. In this
distribution, the mode (mean) represents the relative separation between
the stimuli, and its dispersion indicates the variability of that
separation.

Figure~\ref{fig-discriminal_difference} illustrates the distribution of
the discriminal difference for the hypothetical texts depicted in
Figure~\ref{fig-discriminal_process}. The figure indicates that the
judge perceives Text B as having significantly higher quality than Text
A. This conclusion is supported by two key observations: the positive
difference between their modal discriminal processes
\((T_{B} - T_{A} > 0)\) and the probability area where the discriminal
difference distinctly favors Text B over Text A, represented by the
shaded gray area denoted as \(P(B > A)\). As a result, the dichotomous
outcome of this comparison is more likely to favor Text B over Text A.

\section{Two prominent issues in CJ literature}\label{sec-theory-issues}

This section identifies and discusses the two prominent issues that can
undermine the reliability and validity of CJ's trait estimates.
Section~\ref{sec-theory-issue1} examines the the excessive reliance on
Thurstone's Case V assumptions in the statistical analysis of CJ data.
Section~\ref{sec-theory-issue2} focuses on the apparent disconnect
between CJ's approach to trait measurement and hypothesis testing.

\subsection{The Case V and the statistical analysis of CJ
data}\label{sec-theory-issue1}

Thurstone noted from the outset that the general form of the theory, as
outlined in Section~\ref{sec-thurstone_theory}, gave rise to a trait
scaling problem. Specifically, the model required estimating more
``unknown'' parameters than the available pairwise comparisons
\citep[pp.~267]{Thurstone_1927b}. To address this issue and facilitate
the practical implementation of the theory, he developed five cases
derived from this general form, each progressively incorporating
additional simplifying assumptions into the model.

In Case I, Thurstone postulated that pairs of stimuli would maintain a
constant correlation across all comparisons. In Case II, he allowed
multiple judges to undertake comparisons instead of confining
evaluations to a single judge. In Case III, he posited that there was no
correlation between stimuli. In Case IV, he assumed that the stimuli
exhibited similar dispersions. Finally, in Case V, he replaced this
assumption with the condition that stimuli had equal discriminal
dispersions. Table~\ref{tbl-thurstone_cases} summarizes the assumptions
of the general form and the five cases. For a detailed discussion of
these cases and their progression, refer to \citet{Thurstone_1927b} and
\citet[pp.~248--253]{Bramley_2008}.

\begin{table}

\caption{\label{tbl-thurstone_cases}Thurstones cases and their
asumptions}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/thurstone_cases.png}

}

\end{table}%

Notably, despite relying on the most extensive set of simplifying
assumptions
\citetext{\citealp[pp.~253]{Bramley_2008}; \citealp[pp.~677]{Kelly_et_al_2022}},
Case V remains the most widely used case in the CJ literature. This
popularity stems mainly from its simplified statistical representation
in the Bradley-Terry-Luce (BTL) model
\citep{Bradley_et_al_1952, Luce_1959}. The BTL model mirrors the
assumptions of Case V, with one notable distinction: whereas Case V
assumes a Normal distribution for the stimuli's discriminal processes,
the BTL model uses the more mathematically tractable Logistic
distribution \citep[pp.~254]{Andrich_1978, Bramley_2008} (see
Table~\ref{tbl-thurstone_cases}). This substitution has little impact on
the model's estimation or interpretation, as the Normal and Logistic
distributions exhibit analogous statistical properties, differing only
by a scaling factor of approximately \(1.7\)
\citep[pp.~16]{vanderLinden_et_al_2017_I}.

However, Thurstone originally developed Case V to provide a ``rather
coarse scaling'' of traits \citep[pp.~269]{Thurstone_1927b},
prioritizing statistical simplicity over precision in trait measurement
\citep[pp.~677]{Kelly_et_al_2022}. He explicitly warned against its
untested application, stating that its use ``should not be made without
(an) experimental test'' \citep[pp.~270]{Thurstone_1927b}. Furthermore,
he acknowledged that some assumptions could prove problematic when
researchers assess complex traits or heterogeneous stimuli
\citep[pp.~376]{Thurstone_1927a}. Consequently, given that modern CJ
applications frequently involve such traits and stimuli, two main
assumptions of Case V and, by extension, of the BTL model may not
consistently hold in theory or practice, namely the assumption of equal
dispersion and zero correlation between stimuli.

\subsubsection{The assumption of equal dispersions between
stimuli}\label{sec-theory-issue1a}

According to the theory, discrepancies in the discriminal dispersions of
stimuli shape the distribution of the discriminal difference, exerting a
direct influence on the outcome of pairwise comparisons. A thought
experiment can illustrate this concept. In this experiment, the
researcher observes the discriminal processes for the texts depicted in
Figure~\ref{fig-discriminal_process}. Furthermore, the experiment
assumes that the discriminal dispersion for Text A remains constant and
that the texts are uncorrelated \((\rho=0)\).
Figure~\ref{fig-dispersion} reveals that an increase in the uncertainty
associated with the perception of Text B in comparison to Text A,
\((\sigma_{B}-\sigma_{A})\), broadens the distribution of their
discriminal difference. This broadening affects the probability area
where the discriminal difference distinctly favors Text B over Text A,
expressed as \(P(B > A)\), ultimately influencing the comparison
outcome. Additionally, the figure reveals that when the discriminal
dispersions of the texts are equal \((\sigma_{B}-\sigma_{A}=0)\), the
discriminal difference is more narrow compared to situations where their
dispersions differ. Consequently, the discriminal difference is more
likely to favor Text B over Text A (shaded gray area)

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/dispersion.png}

}

\subcaption{\label{fig-dispersion}Discrepancies in the dispersions of
stimuli}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/correlation.png}

}

\subcaption{\label{fig-correlation}Correlation between stimuli}

\end{minipage}%

\caption{\label{fig-casev_issues}The effect of dispersion discrepancies
and stimulus correlation on the distribution of the discriminal
difference.}

\end{figure}%

In experimental practice, however, the thought experiment occurs in
reverse. Researchers first observe the comparison outcome and then use
the BTL model to infer the discriminal difference between the stimuli
and their respective discriminal processes
\citep[pp.~373]{Thurstone_1927a}. Therefore, the outcome's ability to
reflect the ``true'' differences between stimuli largely depends on the
validity of the model's assumptions \citep[pp.~150]{Kohler_et_al_2019},
particularly the assumption of equal dispersions. For instance, when
researchers observe a sample of outcomes favoring Text B over Text A and
correctly assume equal dispersions between the texts, the BTL model
estimates a discriminal difference distribution that accurately
represents the ``true'' discriminal difference of the texts. This
scenario is illustrated in Figure~\ref{fig-dispersion}, when the model's
discriminal difference distribution aligns with the ``true''
distribution, represented by the thick continuous line corresponding to
\(\sigma_{B}-\sigma_{A}=0\). The accuracy of the discriminal difference
then ensures reliable estimates for the texts' discriminal processes
{(citation needed?)}.

However, Thurstone argued that the assumption of equal dispersions may
not be applicable when researchers assess complex traits or
heterogeneous stimuli \citep[pp.~376]{Thurstone_1927a}, as these traits
and stimuli can introduce judgment discrepancies due to their unique
features
\citep{vanDaal_et_al_2016, Lesterhuis_2018, Chambers_et_al_2022}.
Indeed, evidence of this violation may already be present in the CJ
literature in the form of misfit statistics, which measure judgment
discrepancies associated with specific stimuli
\citetext{\citealp[pp.~12]{Pollitt_2004}; \citealp[pp.~20]{Goossens_et_al_2018}}.
For example, labeling texts as ``misfits'' indicates that comparisons
involving these texts result in more judgment discrepancies than those
involving other texts
\citep{Pollitt_2012a, Pollitt_2012b, vanDaal_et_al_2016, Goossens_et_al_2018}.
These discrepancies, in turn, suggest that the discriminal differences
for ``misfit'' texts have broader distributions, indicating that their
discriminal processes may also exhibit more variation than that of other
texts. A similar line of reasoning applies to the concept of ``misfit''
judges, whose evaluations deviate substantially from the shared
consensus due to the unique characteristics of the stimuli or the judges
themselves. These ``misfit'' judges and their associated deviations can
give rise to additional statistical and measurement issues, which we
discuss in more detail in Section~\ref{sec-theory-issue1b}.

Thus, erroneously assuming equal dispersions between stimuli, can give
rise to significant statistical and measurement issues. For instance,
the model may overestimate the degree to which the outcome accurately
reflects the ``true'' discriminal differences between stimuli. This
overestimation can result in researchers drawing spurious conclusions
about these differences \citep[pp.~370]{McElreath_2020} and, by
extension, about the underlying discriminal processes of stimuli.
Figure~\ref{fig-dispersion} also illustrates this issue when the model's
discriminal difference distribution aligns with the thick continuous
line for \(\sigma_{B}-\sigma_{A}=0\), while the ``true'' discriminal
difference follows any discontinuous line where
\(\sigma_{B}-\sigma_{A} \neq 0\). Additionally, if researchers recognize
that misfit statistics highlight these critical differences in
dispersions, the conventional CJ practice of excluding stimuli based on
these statistics
\citep{Pollitt_2012a, Pollitt_2012b, vanDaal_et_al_2016, Goossens_et_al_2018}
can unintentionally discard valuable information, introducing bias into
trait estimates \citep[chap.~12]{Zimmerman_1994, McElreath_2020}. The
direction and magnitude of these biases are often unpredictable, as they
depend on which stimuli are excluded from the analysis.

\subsubsection{The assumption of zero correlation between
stimuli}\label{sec-theory-issue1b}

The correlation, represented by the symbol \(\rho\), measures how much a
judge's perception of a specific trait in one stimulus depends on their
perception of the same trait in another. As with the discriminal
dispersions, this correlation shapes the distribution of the discriminal
difference, directly impacting the outcomes of pairwise comparisons. A
similar thought experiment, as the one in
Section~\ref{sec-theory-issue1a}, can illustrate this concept. The
experiment now assumes that the discriminal dispersions for both texts
remain constant. Figure~\ref{fig-correlation} reveals that as the
correlation between the texts increases, the distribution of their
discriminal difference becomes narrower. This narrowing affects the area
under the curve where the discriminal difference distinctly favors Text
B over Text A, denoted as \(P(B > A)\), thus influencing the comparison
outcome. Furthermore, the figure shows that when two texts are
independent or uncorrelated \((\rho=0)\), their discriminal difference
is less narrow compared to scenarios where the texts are highly
correlated. Consequently, the discriminal difference is less likely to
favor Text B over Text A (shaded gray area).

Again, in experimental practice, researchers approach this process in
reverse. They begin by observing the sample of outcomes favoring Text B
over Text A and then use the BTL model to estimate the discriminal
difference and the discriminal processes of the stimuli. Given that the
BTL model assumes independent discriminal processes across comparisons,
if this assumption holds, then the model estimates a discriminal
difference distribution that accurately reflects the ``true''
discriminal difference of the texts. This scenario is also illustrated
in Figure~\ref{fig-correlation} when the discriminal difference
distribution of the model aligns with the ``true'' distribution,
represented by the thick continuous line corresponding to \(\rho=0\).
Once more, the accuracy of the discriminal difference ensures reliable
estimates for the discriminal processes of the texts {(citation
needed?)}.

Notably, Thurstone attributed the lack of correlation between stimuli to
the cancellation of potential judges' biases. He argued that this
cancellation resulted from two opposing and equally weighted effects
occurring during pairwise comparisons \citep[pp.~268]{Thurstone_1927b}.
\citet{Andrich_1978} provided a mathematical demonstration of this
cancellation using the BTL model under the assumption of discriminal
processes with additive biases. However, it is easy to imagine at least
two scenarios in which the zero correlation assumption is almost
certainly invalid: when the pairwise comparison involves
multidimensional, complex traits with heterogeneous stimuli and when an
additional hierarchical structure is relevant to the stimuli.

In the first scenario, the intricate aspects of multidimensional,
complex traits may introduce dependencies between the stimuli due to
certain judges' biases that resist cancellation. Research on text
quality suggests that when judges evaluate these traits, they often rely
on various intricate characteristics of the stimuli to form their
judgments
\citep{vanDaal_et_al_2016, Lesterhuis_2018, Chambers_et_al_2022}. These
additional relevant characteristics are unlikely to be equally weighted
or opposing. As a result, they may exert an uneven influence on judges'
perceptions, creating biases in their judgments. Furthermore, since the
discriminal difference of the stimuli becomes an observable outcome only
through the judges' perceptions, these biases may ultimately introduce
dependencies between the stimuli
\citep[pp.~346]{vanderLinden_et_al_2017_II}. For example, this could
occur when a judge assessing the argumentative quality of a text places
more weight on its grammatical accuracy than other judges, thereby
favoring texts with fewer errors but weaker arguments. While direct
evidence for this particular scenario is lacking, studies such as
\citet{Pollitt_et_al_2003} demonstrate the presence of such biases,
supporting the notion that the factors influencing pairwise comparisons
may not always cancel out.

In the second scenario, the shared context or inherent connections
created by additional hierarchical structures may further introduce
dependencies between stimuli, creating a statistical phenomenon known as
clustering \citep{Everitt_et_al_2010}. Although the CJ literature
acknowledges the existence of such hierarchical structures, the
statistical handling of this additional source of dependence between
stimuli has been inadequate. For instance, when CJ data incorporates
multiple samples of stimuli from the same individuals, researchers
frequently rely on (average) estimated BTL scores to conduct subsequent
analyses and tests at the individual hierarchical level
\citep{Bramley_et_al_2019, Boonen_et_al_2020, Bouwer_et_al_2023, vanDaal_et_al_2017, Jones_et_al_2019, Gijsen_et_al_2021}.
This approach, however, can introduce additional statistical and
measurement issues, which we discuss in greater detail in
Section~\ref{sec-theory-issue2}.

In any case, similar to Section~\ref{sec-theory-issue1a}, assuming zero
correlation between stimuli by neglecting additional relevant traits,
excluding judges based on misfit statistics, or ignoring hierarchical
(grouping) structures can cause significant statistical and measurement
issues. In general, the model may over- or underestimate how accurately
the outcome reflects the ``true'' discriminal differences between
stimuli. Such inaccuracies can result in spurious inferences about these
differences and, by extension, about the stimuli's discriminal
processes. This scenario is illustrated by Figure~\ref{fig-correlation},
when the model's discriminal difference distribution aligns with the
thick continuous line for \(\rho=0\), while the ``true'' discriminal
difference follows any discontinuous line where \(\rho \neq 0\).

In particular, neglecting relevant traits, such as judges' biases, can
cause dimensional mismatches in the BTL model, artificially inflating
the trait's reliability \citep[pp.~341]{Hoyle_et_al_2023} or, worse,
introducing bias into the trait's estimates \citep{Ackerman_1989}.
Excluding judges based on misfit statistics risks discarding valuable
information, which may further bias the trait's estimates
\citep[chap.~12]{Zimmerman_1994, McElreath_2020}. Finally, ignoring
hierarchical structures may reduce the precision of model parameter
estimates, potentially overestimating the trait's reliability
\citep[pp.~482]{Hoyle_et_al_2023}.

\subsection{The disconnect between trait measurement and hypothesis
testing}\label{sec-theory-issue2}

Building on the previous section, it is clear that, despite its
limitations, the BTL model is commonly used as a measurement model in CJ
assessments. A measurement model specifies how manifest variables
contribute to the estimation of latent variables
\citep{Everitt_et_al_2010}. For example, when evaluating writing
quality, researchers use the BTL model to process the dichotomous
outcomes resulting from the pairwise comparisons (the manifest
variables) to estimate scores that reflect the underlying level of
writing quality (the latent variable)
\citep{Laming_2004, Pollitt_2012b, Whitehouse_2012, vanDaal_et_al_2016, Lesterhuis_2018_thesis, Coertjens_et_al_2017, Goossens_et_al_2018, Bouwer_et_al_2023}.

Researchers then typically use these estimated BTL scores, or their
transformations, to conduct additional analyses or hypothesis tests. For
example, these scores have been used to identify `misfit' judges and
stimuli \citep{Pollitt_2012b, vanDaal_et_al_2016, Goossens_et_al_2018},
detect biases in judges' ratings
\citep{Pollitt_et_al_2003, Pollitt_2012b}, calculate correlations with
other assessment methods \citep{Goossens_et_al_2018, Bouwer_et_al_2023},
or test hypotheses related to the underlying trait of interest
\citep{Bramley_et_al_2019, Boonen_et_al_2020, Bouwer_et_al_2023, vanDaal_et_al_2017, Jones_et_al_2019, Gijsen_et_al_2021}.

However, the statistical literature advises caution when using estimated
scores for additional analyses and tests. A key consideration is that
BTL scores are parameter estimates that inherently carry uncertainty.
Ignoring this uncertainty can bias the analysis and reduce the precision
of hypothesis tests. Notably, the direction and magnitude of such biases
are often unpredictable. Results may be attenuated, exaggerated, or
remain unaffected depending on the degree of uncertainty in the scores
and the actual effects being tested
\citetext{\citealp[pp.~25]{Kline_et_al_2023}; \citealp[pp.~137]{Hoyle_et_al_2023}}.
Finally, the reduced precision in hypothesis tests diminishes their
statistical power, increasing the likelihood of committing type-I or
type-II errors \citep{McElreath_2020}.

In aggregate, the excessive reliance on Thurstone's Case V assumptions
in the statistical analysis of comparative data can compromise the
reliability of the trait estimates. This overreliance may also undermine
their validity \citep[pp.~2]{Perron_et_al_2015}, especially when
combined with the disconnect between CJ's approach to trait measurement
and hypothesis testing. However, the structural approach to causal
inference can address these issues by providing a systematic and
integrated framework that enhances statistical accuracy while
strengthening measurement reliability and validity.

\section{Extending Thurstone's general form}\label{sec-theoretical}

The \emph{structural approach} to causal inference provides a formal
framework for identifying causes and estimating their effects using
data. The approach uses structural causal models (SCMs) and directed
acyclic graphs (DAGs)
\citep{Pearl_2009, Pearl_et_al_2016, Gross_et_al_2018, Neal_2020} to
formally and graphically represent the assumed causal structure of a
system, such as the one found in CJ experiments.

Essentially, SCMs and DAGs function as \emph{conceptual models} on which
identification analysis rests \citep[pp.~4]{Schuessler_et_al_2023}.
\emph{Identification analysis} helps researchers to determine whether an
estimator can accurately compute an estimand based solely on its
(causal) assumptions, regardless of random variability
\citep[pp.~4]{Schuessler_et_al_2023}. Here, \emph{estimands} represent
the specific quantities researchers aim to determine
\citep{Everitt_et_al_2010}. \emph{Estimators} denote the methods or
functions that transform data into an estimate, while \emph{estimates}
are the numerical values approximating the estimand
\citep{Neal_2020, Everitt_et_al_2010}.

A motivating example helps to clarify these concepts. This example will
appear throughout the document to illustrate various aspects of the
proposed development. In this example, researchers aim to determine:
``To what extent do different teaching methods influence students'
ability to produce high-quality written texts?'' To investigate this,
researchers could design a CJ experiment by randomly assigning students
(individuals) to two groups, each receiving a different teaching method.
Judges would then compare pairs of students' written texts (stimuli), to
produce a dichotomous outcome that reflects the relative quality of each
text (trait). Based on this setup, researchers could reformulate the
research question as the estimand: ``\emph{On average}, is there a
difference in the ability to produce high-quality written texts between
the two groups of students?''. Finally, following current CJ practices,
researchers would then rely on estimates from the BTL model, or its
transformations, to approximate this estimand.

However, Section~\ref{sec-theory-issues} presents compelling evidence
that the BTL model has several statistical and measurement limitations.
These limitations hinder the model's ability to identify various
estimands relevant to CJ inquiries, including the one described in the
motivating example. Identification is crucial because it is a necessary
condition for ensuring consistent estimators. \emph{Consistency} refers
to the property of an estimator whose estimates converge to the ``true''
value of the estimand as the data size approaches infinity
\citep{Everitt_et_al_2010}. Without identification, consistency cannot
be achieved, even with ``infinite'' and error-free data. Consequently,
deriving meaningful insights from finite data becomes impossible
\citep[pp.~5]{Schuessler_et_al_2023}.

Luckily, SCMs and DAGs support identification analysis through two key
advantages. First, regardless of complexity, they can represent various
causal structures using only five fundamental building blocks
\citep{Neal_2020, McElreath_2020}. This feature allows researchers to
decompose complex structures into manageable components, facilitating
their analysis \citep{McElreath_2020}. Second, they depict causal
relationships in an interactive, non-parametric way. This flexibility
enables feasible identification strategies without requiring
specification of the types of variables, the functional forms relating
them, or the parameters of those functional forms
\citep[pp.~35]{Pearl_et_al_2016}.

Thus, this section addresses the identified issues in
Section~\ref{sec-theory-issues} by extending Thurstone's general form
using the structural approach to causal inference. Specifically, it
leverages the capabilities of this approach to formalize the combination
of the core theoretical principles outlined in
Section~\ref{sec-thurstone_theory} with key assessment design features
relevant to CJ experiments, such as the selection of judges, stimuli,
and comparisons. In addition to enhancing statistical accuracy and
strengthening measurement reliability and validity, the approach offers
two key advantages. First, it clarifies the interactions among all
actors and processes involved in CJ experiments. Second, it shifts the
current comparative data analysis paradigm from passively accepting the
model assumptions to actively testing whether those assumptions fit the
data under analysis.

Accordingly, Section~\ref{sec-theory-theoretical_P} incorporates the
theoretical principles into what we call the \emph{conceptual-population
model}. This model assumes that researchers have access to a conceptual
population of comparison data, that is, data representing all repeated
comparisons made by every available judge for each pair of stimuli
produced by each pair of individuals in the population, hence its name.
Conversely, Section~\ref{sec-theory-theoretical_SC} integrates the
assessment design features into what we call the \emph{sample-comparison
model}. This model assumes a more realistic scenario where researchers
only have access to a sample of judges, individuals, stimuli, and
comparisons from the conceptual population.

\subsection{The conceptual-population
model}\label{sec-theory-theoretical_P}

Before incorporating the theoretical principles into the
\emph{conceptual-population model}, it is essential to define SCMs. SCMs
are formal mathematical models characterized by a set of
\emph{endogenous} variables \(V\), a set of \emph{exogenous} variables
\(E\), and a set of functions \(F\)
\citep{Pearl_2009, Cinelli_et_al_2020}. Endogenous variables are those
whose causal mechanisms a researcher chooses to model \citep{Neal_2020}.
In contrast, exogenous variables represent \emph{errors} or
\emph{disturbances} arising from omitted factors that the investigator
chooses not to model explicitly \citep[pp.~27,68]{Pearl_2009}. Lastly,
the functions, referred to as \emph{structural equations}, express the
endogenous variables as non-parametric functions of other endogenous and
exogenous variables. These functions use the symbol `\(:=\)' to denote
the asymmetrical causal dependence between variables and the symbol
`\(\:\bot\:\)' to represent \emph{d-separation}, a concept akin to
(conditional) independence.

Figure~\ref{fig-cj04} illustrates the preliminary integration of
theoretical principles into the conceptual-population model.
Specifically, Figure~\ref{fig-cj04_scm} shows the SCM that underlies the
motivating example. This SCM maps the relationship between the
conceptual-population outcome \((O^{cp}_{abhijk})\) to several other
variables. In this SCM, the subscripts \(\{a,b\}\) represent the
compared texts (stimuli), with \(a \neq b\). The indices \(\{h,i\}\)
label the students who wrote the texts (individuals), where \(h = i\) is
possible. Moreover, the index \(j\) denotes the judge, and \(k\) denotes
the judgment number, which accounts for experimental cases where judges
perform multiple judgments of the same texts written by the same
students, i.e., a \emph{repeated measures design}
\citep[pp.~366-376]{Lawson_2015}.

The other variables in SCM~\ref{fig-cj04_scm} include the texts'
discriminal processes \((T_{ahjk}, T_{bijk})\) and their discriminal
difference \((D_{abhijk})\), in line with Thurstone's theory (see
Section~\ref{sec-thurstone_theory}). Additionally, based on the evidence
presented in Section~\ref{sec-theory-issue1b} and the recommendations of
\citet{Andrich_1978} and \citet{Wainer_et_al_1978}, we incorporate the
judges' biases \((B_{kj})\) into the model. Together with the outcome,
these variables form the preliminary set of endogenous variables,
\(V = \{ O_{abhijk}, D_{abhijk}, T_{ahjk}, T_{bijk}, B_{kj} \}\).
Finally, the SCM shows the preliminary set of structural equations,
\(F = \{ f_{O}, f_{D} \}\), which define the non-parametric dependencies
among these variables.

By incorporating judges' biases into the conceptual-population model, we
assume that additional relevant characteristics of the stimuli are
unlikely to be equally weighted or opposing. Thus, these characteristics
may exert an uneven influence on judges' perceptions, creating biases in
their judgments. Furthermore, since the discriminal difference of the
stimuli becomes an observable outcome only through the judges'
perceptions, these biases may ultimately introduce dependencies between
the stimuli \citep[pp.~346]{vanderLinden_et_al_2017_II}, hence the need
for their integration. The full statistical elaboration of this point is
presented in Section~\ref{sec-statistical}.

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{cp}_{abhijk} & := f_{O}(D_{abhijk}) \\
  D_{abhijk} & := f_{D}(T_{ah}, T_{bi}, B_{jk})
\end{aligned}
\]

}

\subcaption{\label{fig-cj03_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.35\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_03.png}

}

\subcaption{\label{fig-cj03_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj03}CJ's outcome, discriminal difference,
discriminal processes and judges' biases}

\end{figure}%

Notably, every SCM has an associated DAG
\citep{Pearl_et_al_2016, Cinelli_et_al_2020}. A DAG is a graph
consisting of nodes connected by edges, where the nodes represent random
variables. The term \emph{directed} means that the edges extend from one
node to another, with arrows indicating the direction of causal
influence. The term \emph{acyclic} implies that the causal influences do
not form loops, ensuring the influences do not cycle back on themselves
\citep{McElreath_2020}. DAGs represent observed variables as solid black
circles, while they use open circles for unobserved (latent) variables
\citep{Morgan_et_al_2014}. While the \emph{standard representation} of
DAGs typically omits exogenous variables for simplicity, the
\emph{magnified representation} includes them, offering the advantage of
highlighting potential issues related to conditioning and confounding
\citep{Cinelli_et_al_2020}. Thus, this study uses the magnified
representation.

Figure~\ref{fig-cj04_dag} displays the DAG associated with
SCM~\ref{fig-cj03_scm}. The graph shows the direction of causal
influences among variables, reflecting the assumptions of Thurstone's
theory and CJ experiments. It also distinguishes between observable
endogenous variables, such as the outcome, and variables considered
latent according to the theory and practice of CJ, such as the the
text's discriminal processes, their discriminal difference, and the
judges' biases.

It is easy to see that representing the CJ system using indices for a
single pair of individuals and stimuli, along with the indices for the
judgment number and the judge responsible for the comparison, can
quickly become intractable and cumbersome. Thus, to maintain generality
and simplify the notation, certain indices are grouped into vectors or
matrices. First, we define the matrices \(IA\) and \(JK\) as:

\[
IA = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
\vdots & \vdots \\
h & a \\
\vdots & \vdots \\
i & b \\
\vdots & \vdots \\
n_{I} & n_{A}-1 \\
n_{I} & n_{A} \\
\end{bmatrix} \in \mathbb{N}^{(n_{I} \cdot n_{A}) \times 2} \; ; \quad
JK = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
\vdots & \vdots \\
j & k \\
\vdots & \vdots \\
n_{J} & n_{K}-1 \\
n_{I} & n_{A} \\
\end{bmatrix} \in \mathbb{N}^{(n_{J} \cdot n_{K}) \times 2}
\]

Each row of \(IA\) represents a unique pairing of an individual
\(\{h,i\}\) with a stimulus \(\{a,b\}\). Here, \(n_{I}\) represents the
number of individuals and \(n_{A}\) the number of stimulus within
individuals in the population. Similarly, each row of \(JK\) represents
a unique pairing of a judge \(j\) with a judgment \(k\). Here, \(n_{J}\)
represents the number of judges and \(n_{K}\) the number of judgments
performed by each judge in the population.

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{cp}_{RPUV} & := f_{O}(D_{RPUV}) \\
  D_{RPUV} & := f_{D}(T_{IA}, B_{JK})
\end{aligned}
\]

}

\subcaption{\label{fig-cj04_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.44\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_04.png}

}

\subcaption{\label{fig-cj04_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj04}CJ's outcome, discriminal difference,
discriminal processes and judges' biases}

\end{figure}%

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{cp}_{RPUV} & := f_{O}(D_{RPUV}) \\
  D_{RPUV} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{A}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{K}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) 
\end{aligned}
\]

}

\subcaption{\label{fig-cj09_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_09.png}

}

\subcaption{\label{fig-cj09_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj09}CJ's outcome, discriminal difference,
discriminal processes and judges' biases}

\end{figure}%

\subsection{The sample-comparison
model}\label{sec-theory-theoretical_SC}

Considering the sampling mechanism

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{sc}_{RPUV} & := f_{O}(O^{cp}_{RPUV}, S_{I}, S_{A}, S_{K}, S_{J}) \\
  O^{cp}_{RPUV} & := f_{O}(D_{RPUV}) \\
  D_{RPUV} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{A}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{K}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) 
\end{aligned}
\]

}

\subcaption{\label{fig-cj11_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.84\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_11.png}

}

\subcaption{\label{fig-cj11_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj11}CJ's outcome, discriminal difference,
discriminal processes and judges' biases}

\end{figure}%

Considering comparison mechanisms

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O_{RPUV} & := f_{C}(O^{sc}_{RPUV}, C) \\
  O^{sc}_{RPUV} & := f_{O}(O^{cp}_{RPUV}, S_{I}, S_{A}, S_{K}, S_{J}) \\
  O^{cp}_{RPUV} & := f_{O}(D_{RPUV}) \\
  D_{RPUV} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{A}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{K}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) 
\end{aligned}
\]

}

\subcaption{\label{fig-cj13_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_13.png}

}

\subcaption{\label{fig-cj13_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj13}CJ's outcome, discriminal difference,
discriminal processes and judges' biases}

\end{figure}%

\section{Abandoning the BTL model}\label{sec-statistical}

\section{Discussion}\label{sec-discussion}

\subsection{Findings}\label{sec-discussion-finding}

\subsection{Limitations and further
research}\label{sec-discussion-limitations}

\section{Conclusion}\label{sec-conclusion}

\newpage{}

\section*{Declarations}\label{declarations}
\addcontentsline{toc}{section}{Declarations}

\textbf{Funding:} The Research Fund (BOF) of the University of Antwerp
funded this project.

\textbf{Financial interests:} The authors declare no relevant financial
interests.

\textbf{Non-financial interests:} The authors declare no relevant
non-financial interests.

\textbf{Ethics approval:} The University of Antwerp Research Ethics
Committee confirmed that this study does not require ethical approval.

\textbf{Consent to participate:} Not applicable

\textbf{Consent for publication:} All authors have read and approved the
final version of the manuscript for publication.

\textbf{Data availability:} This study did not use any data.

\textbf{Materials and code availability:} The \texttt{CODE\ LINK}
section at the top of the digital document located at:
\url{https://jriveraespejo.github.io/paper2_manuscript/} provides access
to all materials and code.

\textbf{AI-assisted technologies in the writing process:} The authors
used various AI-based language tools to refine phrasing, optimize
wording, and enhance clarity and coherence throughout the manuscript.
They take full responsibility for the final content of the publication.

\textbf{CRediT authorship contribution statement:}
\emph{Conceptualization:} S.G., S.DM., T.vD., and J.M.R.E;
\emph{Methodology:} S.DM., T.vD., and J.M.R.E; \emph{Software:}
J.M.R.E.; \emph{Validation:} J.M.R.E.; \emph{Formal Analysis:} J.M.R.E.;
\emph{Investigation:} J.M.R.E; \emph{Resources:} S.G., S.DM., and T.vD.;
\emph{Data curation:} J.M.R.E.; \emph{Writing - original draft:}
J.M.R.E.; \emph{Writing - review and editing:} S.G., S.DM., and T.vD.;
\emph{Visualization:} J.M.R.E.; \emph{Supervision:} S.G. and S.DM.;
\emph{Project administration:} S.G. and S.DM.; \emph{Funding
acquisition:} S.G. and S.DM.

\newpage{}

\section{Appendix}\label{sec-appendix}

\subsection{Statistics and Causal inference}\label{sec-appendixB}

This section introduces fundamental statistical and causal inference
concepts necessary for understanding the core theoretical principles
described in this document. It does not, however, offer a comprehensive
overview of causal inference methods. Readers seeking more in-depth
understanding may wish to explore introductory papers such as
\citet{Pearl_2010}, \citet{Rohrer_2018}, \citet{Pearl_2019}, and
\citet{Cinelli_et_al_2020}. They may also find it helpful to consult
introductory books like \citet{Pearl_et_al_2018}, \citet{Neal_2020}, and
\citet{McElreath_2020}. For more advanced study, readers may refer to
seminal intermediate papers such as \citet{Neyman_et_al_1923},
\citet{Rubin_1974}, \citet{Spirtes_et_al_1991}, and \citet{Sekhon_2009},
as well as books such as \citet{Pearl_2009}, \citet{Morgan_et_al_2014},
and \citet{Hernan_et_al_2020}.

\subsubsection{Empirical research and randomized
experiments}\label{sec-appendixB1}

Empirical research uses evidence from observation and experimentation to
address real-world challenges. In this context, researchers typically
formulate their research questions as \emph{estimands} or \emph{targets
of inference}, i.e., the specific quantities they seek to determine
\citep{Everitt_et_al_2010}. For instance, researchers might be
interested in answering the following question: ``To what extent do
different teaching methods \((T)\) influence students' ability to
produce high-quality written texts \((Y)\)?'' To investigate this,
researchers could randomly assign students to two groups, each exposed
to a different teaching method \((T_{i} = \{1,2\})\). Then, they would
perform pairwise comparisons, generating a dichotomous outcome
\((Y_{i} = \{0,1\})\) showing which student exhibits more of the
ability. In this scenario, the research question can be rephrased as the
estimand, ``\emph{On average}, is there a difference in the ability to
produce high-quality written texts between the two groups of students?''
and this estimand can be mathematically represented by the random
associational quantity in Equation~\ref{eq-group_diff}, where
\(E[\cdot]\) denotes the expected value.

\begin{equation}\phantomsection\label{eq-group_diff}{
E[Y_{i} \:|\:T_{i}=1] - E[Y_{i} \:|\:T_{i}=2]
}\end{equation}

Researchers then proceed to identify the estimands.
\emph{Identification} determines whether an estimator can accurately
compute the estimand based solely on its assumptions, regardless of
random variability \citep[pp.~4]{Schuessler_et_al_2023}. An
\emph{estimator} refers to a method or function that transforms data
into an estimate \citep{Neal_2020}. \emph{Estimates} are numerical
values that approximate the estimand derived through the process of
\emph{estimation}, which integrates data with an estimator
\citep{Everitt_et_al_2010}. The Identification-Estimation flowchart
\citep{McElreath_2020, Neal_2020}, shown in Figure~\ref{fig-IEflow},
visually represents the transition from estimands to estimates.

\begin{figure}

\centering{

\includegraphics[width=0.35\linewidth,height=\textheight,keepaspectratio]{images/png/IEflow.png}

}

\caption{\label{fig-IEflow}Identification-Estimation flowchart.
Extracted and slightly modified from \citet[pp.~32]{Neal_2020}}

\end{figure}%

Identification is a necessary condition to ensure \emph{consistent}
estimators. An estimator achieves \emph{consistency} when it converges
to the ``true'' value of an estimand as the data size approaches
infinity \citep{Everitt_et_al_2010}. Without identification, researchers
cannot achieve consistency, even with ``infinite'' and error-free data.
As a result, deriving meaningful insights about an estimand from finite
data becomes impossible \citep[pp.~5]{Schuessler_et_al_2023}. Therefore,
to ensure accurate and reliable estimates, researchers prioritize
estimators with desirable identification properties. For instance, the
Z-test is a widely used estimator for comparing group proportions,
yielding accurate estimates when its underlying assumptions are
satisfied \citep{Kanji_2006}. Furthermore, researchers can interpret
estimates from the Z-test as causal, provided the data is collected
through a randomized experiment.

Randomized experiments are widely recognized as the gold standard in
evidence-based science \citep{Hariton_et_al_2018, Hansson_2014}. This
recognition stems from their ability to enable researchers interpret
associational estimates as causal. They achieve this by ensuring data,
and by extension an estimator, satisfies several key identification
properties, such as common support, no interference, and consistency
\citep{Morgan_et_al_2014, Neal_2020}. The most critical property,
however, is the elimination of confounding. \emph{Confounding} occurs
when an external variable \(X\) simultaneously influences the outcome
\(Y\) and the variable of interest \(T\), resulting in spurious
associations \citep{Everitt_et_al_2010}. Randomization addresses this
issue by decoupling the association between the intervention allocation
\(T\) from any other variable \(X\)
\citep{Morgan_et_al_2014, Neal_2020}.

Nevertheless, researchers often face constraints that limit their
ability to conduct randomized experiments. These constraints include
ethical concerns, such as the assignment of individuals to potentially
harmful interventions, and practical limitations, such as the
infeasibility of, for example, assigning individuals to genetic
modifications or physical impairments \citep{Neal_2020}. In these cases,
causal inference offers a valuable alternative for generating causal
estimates and understanding the mechanisms underlying specific data. In
addition, the framework can provide significant theoretical insights
that can enhance the design of experimental and observational studies
\citep{McElreath_2020}.

\subsubsection{Identification under causal
inference}\label{sec-appendixB2}

Unlike classical statistical modeling, which focuses primarily on
summarizing data and inferring associations, the \emph{causal inference}
framework is designed to identify causes and estimate their effects
using data \citep{Shaughnessy_et_al_2010, Neal_2020}. The framework uses
rigorous mathematical techniques to address the \emph{fundamental
problem of causality}
\citep{Pearl_2009, Pearl_et_al_2016, Morgan_et_al_2014}. This problem
revolves around the question, ``What would have happened `in the world'
under different circumstances?'' This question introduces the concept of
counterfactuals, which are instrumental in defining and identifying
causal effects.

\emph{Counterfactuals} are hypothetical scenarios that are
\emph{contrary to fact}, where alternative outcomes resulting from a
given cause are neither observed nor observable
\citep{Neal_2020, Counterfactual_2024}. The structural approach to
causal inference \citep{Pearl_2009, Pearl_et_al_2016} provides a formal
framework for defining counterfactuals. For instance, in the scenario
described in Section~\ref{sec-appendixB1}, the approach begins by
defining the \emph{individual causal effect} (ICE) as the difference
between each student's potential outcomes, as in Equation~\ref{eq-ICE}.

\begin{equation}\phantomsection\label{eq-ICE}{
\tau_{i} = Y_{i} \:|\:do(T_{i}=1) - Y_{i} \:|\:do(T_{i}=2)
}\end{equation}

where \(do(T_{i}=t)\) represents the intervention operator,
\(Y_{i} \:|\:do(T_{i}=1)\) represents the potential outcome under
intervention \(T_{i}=1\), and \(Y_{i} \:|\:do(T_{i}=1)\) represents the
potential outcome under intervention \(T_{i}=2\). Here, an
\emph{intervention} involves assigning a constant value to the treatment
variable for each student's potential outcomes. Note that if a student
is assigned to intervention \(T_{i}=1\), the potential outcome under
\(T_{i}=2\) becomes a counterfactual, as it is no longer observed nor
observable. To address this challenge, the structural approach extends
the ICE to the \emph{average causal effect} (ACE,
Equation~\ref{eq-ACE}), representing the average difference between the
students' observed potential outcomes and their counterfactual
counterparts.

\begin{equation}\phantomsection\label{eq-ACE}{
\begin{aligned}
\tau & = E[\tau_{i}] \\
  & = E[Y_{i} \:|\:do(T_{i}=1)]- E[Y_{i} \:|\:do(T_{i}=2)]
\end{aligned}
}\end{equation}

Even though counterfactuals are unobservable, researchers can still
identify the ACE from associational estimates by leveraging the
structural approach. The approach identifies the ACE by statistically
conditioning data on a \emph{sufficient adjustment set} of variables
\(X\) \citep{Pearl_2009, Pearl_et_al_2016, Morgan_et_al_2014}. This
\emph{sufficient} set (potentially empty) must block all non-causal
paths between \(T\) to \(Y\) without opening new ones. When such a set
exists, then \(T\) and \(Y\) are \emph{d-separated} by \(X\)
(\(T \:\bot\:Y \:|\:X\)) \citep{Pearl_2009}, and \(X\) satisfies the
\emph{backdoor criterion} \citep[pp 37]{Neal_2020}. Here,
\emph{conditioning} describes the process of restricting the focus to
the subset of the population defined by the conditioning variable
\citep[pp.~32]{Neal_2020} (see Equation~\ref{eq-CACE}).

Conditioning on a sufficient adjustment set enables researchers to
estimate the ACE, even when the data comes from an observational study.
This process is feasible because such conditioning ensures that the ACE
estimator satisfies several critical properties, including confounding
elimination \citep{Morgan_et_al_2014}. Naturally, the validity of claims
about the causal effects of \(T\) on \(Y\) now hinges on the assumption
that \(X\) serves as a sufficient adjustment set. However, as
\citet[pp.~150]{Kohler_et_al_2019} noted, drawing conclusions about the
real world from observed data inevitably requires assumptions. This
requirement holds true for both observational and experimental data.

For instance, if researchers cannot conduct the randomized experiments
described in Section~\ref{sec-appendixB1} and must instead rely on
observational data, they can still identify the ACE as long as an
observed variable \(X\), such as the socio-economic status of the
school, satisfies the backdoor criterion. Under these circumstances,
researchers first identify the \emph{conditional average causal effect}
(CACE, Equation~\ref{eq-CACE})

\begin{equation}\phantomsection\label{eq-CACE}{
CACE_{t} = E[Y_{i} \:|\:T_{i}=t, X]
}\end{equation}

From the CACE, researchers can identify the ACE from associational
quantities as in Equation~\ref{eq-mACE1}. This identification process is
commonly known as the \emph{backdoor adjustment}. Here, \(E_{X}[\cdot]\)
represents the marginal expected value over \(X\)
\citep{Morgan_et_al_2014}.

\begin{equation}\phantomsection\label{eq-mACE1}{
\begin{aligned}
  \tau & = E[Y_{i} \:|\:do(T_{i}=1)]- E[Y_{i} \:|\:do(T_{i}=2)] \\
  & = E_{X}[CACE_{1} - CACE_{2}] \\
  & = E_{X}\left[ E[Y_{i} \:|\:T_{i}=1, X] - E[Y_{i} \:|\:T_{i}=2, X] \right]
\end{aligned}
}\end{equation}

Notably, the approach extends the ACE identification for a continuous
variable \(T\) as in Equation~\ref{eq-mACE_cont}, ensuring broad
applicability across different causal scenarios
\citep[pp.~45]{Neal_2020}

\begin{equation}\phantomsection\label{eq-mACE_cont}{
\begin{aligned}
  \tau &= E[Y_{i} \:|\:do(T_{i}=t)] \\
  & = d E_{X}\left[ E[Y_{i} \:|\:T_{i}=t, X]\right]/ dt
  \end{aligned}
}\end{equation}

\subsubsection{Diving into the specifics}\label{sec-appendixB3}

The structural approach to causal inference uses SCMs and DAGs to
formally and graphically represent the presumed causal structure
underlying the ACE
\citep{Pearl_2009, Pearl_et_al_2016, Gross_et_al_2018, Neal_2020}.
Essentially, these tools serve as \emph{conceptual (theoretical) models}
on which identification analysis rests
\citep[pp.~4]{Schuessler_et_al_2023}. Thus, using these tools,
researchers can determine which statistical models can identify (ACE,
CACE, or other), assuming the depicted causal structure is correct
\citep{McElreath_2020}, thus enabling valid causal inference.
Figure~\ref{fig-IEflow} shows the role of theoretical models in the
inference process.

SCMs and DAGs support identification analysis through two key
advantages. First, regardless of complexity, they can represent various
causal structures using only five fundamental building blocks
\citep{Neal_2020, McElreath_2020}. This feature allows researchers to
decompose complex structures into manageable components, facilitating
their analysis \citep{McElreath_2020}. Second, they depict causal
relationships in a non-parametric and fully interactive way. This
flexibility enables feasible ACE identification strategies without
defining the variables' data types, the functional form between them, or
their parameters \citep[pp.~35]{Pearl_et_al_2016}.

Thus, Section~\ref{sec-appendixB31} and Section~\ref{sec-appendixB32}
elaborate on the first advantage, while Section~\ref{sec-appendixB32}
and Section~\ref{sec-appendixB33} do so for the second. Finally,
Section~\ref{sec-appendixB34} explains how researchers use SCMs and DAGs
alongside Bayesian inference methods in the estimation process.

\paragraph{The five fundamental block for SCMs and
DAGs}\label{sec-appendixB31}

Figures \ref{fig-dags_scms1}, \ref{fig-dags_scms2},
\ref{fig-dags_scms3}, \ref{fig-dags_scms4}, and \ref{fig-dags_scms5}
display the five fundamental building blocks for SCMs and DAGs. The left
panels of the figures show the formal mathematical models, represented
by the SCMs, defined in terms of a set of \emph{endogenous} variables
\(V=\{X_{1},X_{2},X_{3}\}\), a set of \emph{exogenous} variables
\(E=\{e_{X1},e_{X2},e_{X3}\}\), and a set of functions
\(F=\{f_{X1},f_{X2},f_{X3}\}\) \citep{Pearl_2009, Cinelli_et_al_2020}.
Endogenous variables are those whose causal mechanisms a researcher
chooses to model \citep{Neal_2020}. In contrast, exogenous variables
represent \emph{errors} or \emph{disturbances} arising from omitted
factors that the investigator chooses not to model explicitly
\citep[pp.~27,68]{Pearl_2009}. Lastly, the functions, referred to as
\emph{structural equations}, express the endogenous variables as
non-parametric functions of other variables. These functions use the
symbol `\(:=\)' to denote the asymmetrical causal dependence of the
variables and the symbol `\(\:\bot\:\)' to represent
\emph{d-separation}, a concept akin to (conditional) independence.

Notably, every SCM has an associated DAG
\citep{Pearl_et_al_2016, Cinelli_et_al_2020}. The right panels of the
figures display these DAGs. A DAG is a graph consisting of nodes
connected by edges, where the nodes represent random variables. The term
\emph{directed} means that the edges extend from one node to another,
with arrows indicating the direction of causal influence. The term
\emph{acyclic} implies that the causal influences do not form loops,
ensuring the influences do not cycle back on themselves
\citep{McElreath_2020}. DAGs represent observed variables as solid black
circles, while they use open circles for unobserved (latent) variables
\citep{Morgan_et_al_2014}. Although the \emph{standard representation}
of DAGs typically omits exogenous variables for simplicity, the
\emph{magnified representation} depicted in the figures offers one key
advantage: including exogenous variables can help researchers highlight
potential issues related to conditioning and confounding
\citep{Cinelli_et_al_2020}.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X_{1} & := f_{X1}(e_{X1}) \\
  X_{3} & := f_{X3}(e_{X3}) \\
  e_{X1} & \:\bot\:e_{X3}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_bb1}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.54\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_bb1.png}

}

\subcaption{\label{fig-mdag_bb1}DAG}

\end{minipage}%

\caption{\label{fig-dags_scms1}Two unconnected nodes}

\end{figure}%

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X_{1} & := f_{X1}(e_{X1}) \\
  X_{3} & := f_{X3}(X_{1},e_{X3}) \\
  e_{X1} & \:\bot\:e_{X3}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_bb2}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.54\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_bb2.png}

}

\subcaption{\label{fig-mdag_bb2}DAG}

\end{minipage}%

\caption{\label{fig-dags_scms2}Two connected nodes or descendant}

\end{figure}%

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X_{1} & := f_{X1}(e_{X1}) \\
  X_{2} & := f_{X2}(X_{1},e_{X2}) \\
  X_{3} & := f_{X3}(X_{2},e_{X3}) \\
  e_{X1} & \:\bot\:e_{X2} \\
  e_{X1} & \:\bot\:e_{X3} \\
  e_{X2} & \:\bot\:e_{X3}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_bb3}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_bb3.png}

}

\subcaption{\label{fig-mdag_bb3}DAG}

\end{minipage}%

\caption{\label{fig-dags_scms3}Chain or mediator}

\end{figure}%

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X_{1} & := f_{X1}(X_{2},e_{X1}) \\
  X_{2} & := f_{X2}(e_{X2}) \\
  X_{3} & := f_{X3}(X_{2},e_{X3}) \\
  e_{X1} & \:\bot\:e_{X2} \\
  e_{X1} & \:\bot\:e_{X3} \\
  e_{X2} & \:\bot\:e_{X3}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_bb4}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_bb4.png}

}

\subcaption{\label{fig-mdag_bb4}DAG}

\end{minipage}%

\caption{\label{fig-dags_scms4}Fork or confounder}

\end{figure}%

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X_{1} & := f_{X1}(e_{X1}) \\
  X_{2} & := f_{X2}(X_{1},X_{3},e_{X2}) \\
  X_{3} & := f_{X3}(e_{X3}) \\
  e_{X1} & \:\bot\:e_{X2} \\
  e_{X1} & \:\bot\:e_{X3} \\
  e_{X2} & \:\bot\:e_{X3}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_bb5}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_bb5.png}

}

\subcaption{\label{fig-mdag_bb5}DAG}

\end{minipage}%

\caption{\label{fig-dags_scms5}Collider or inmorality}

\end{figure}%

A careful examination of these building blocks highlights the
theoretical assumptions underlying their observed variables. SCM
\ref{fig-scm_bb1} and DAG \ref{fig-mdag_bb1} depict two unconnected
nodes, representing a scenario where variables \(X_{1}\) and \(X_{3}\)
are independent or not causally related. SCM \ref{fig-scm_bb2} and DAG
\ref{fig-mdag_bb2} illustrate two connected nodes, representing a
scenario where a \emph{parent} node \(X_{1}\) exerts a causal influence
on a \emph{child} node \(X_{3}\). In this setup, \(X_{3}\) is considered
a \emph{descendant} of \(X_{1}\). Additionally, \(X_{1}\) and \(X_{3}\)
are described as \emph{adjacent} because there is a \emph{direct path}
connecting them. SCM \ref{fig-scm_bb3} and DAG \ref{fig-mdag_bb3} depict
a \emph{chain}, where \(X_{1}\) influences \(X_{2}\), and \(X_{2}\)
influences \(X_{3}\). In this configuration, \(X_{1}\) is a parent node
of \(X_{2}\), which is a parent node of \(X_{3}\). This structure
creates a \emph{directed path} between \(X_{1}\) and \(X_{3}\).
Consequently, \(X_{1}\) is an \emph{ancestor} of \(X_{3}\), and
\(X_{2}\) fully \emph{mediates} the relationship between the two. SCM
\ref{fig-scm_bb4} and DAG \ref{fig-mdag_bb4} illustrate a \emph{fork},
where variables \(X_{1}\) and \(X_{3}\) are both influenced by
\(X_{2}\). Here, \(X_{2}\) is a parent node that \emph{confounds} the
relationship between \(X_{1}\) and \(X_{3}\). Finally, SCM
\ref{fig-scm_bb5} and DAG \ref{fig-mdag_bb5} show a \emph{collider},
where variables \(X_{1}\) and \(X_{3}\) are concurrent causes of
\(X_{2}\). In this configuration, \(X_{1}\) and \(X_{3}\) are not
causally related to each other but both influence \(X_{2}\) (an
\emph{inmorality}). Notably, all building blocks assume the errors are
independent of each other and from all other variables in the graph, as
evidenced by the pairwise relations \(e_{X1} \:\bot\:e_{X2}\),
\(e_{X1} \:\bot\:e_{X3}\), and \(e_{X2} \:\bot\:e_{X3}\).

Researchers can then use these building blocks to represent the scenario
outlined in Section~\ref{sec-appendixB2}. SCM \ref{fig-scm_example1} and
DAG \ref{fig-mdag_example1} depict the plausible causal structure for
this example. In this context, the variable \(X\) (socio-economic status
of the school) is thought to be a confounder in the relationship between
the teaching method \(T\) and the outcome \(Y\). The figures display
multiple descendant relationships such as \(X \rightarrow T\),
\(X \rightarrow Y\), and \(T \rightarrow Y\). They also highlight
unconnected node pairs, evident from the relationships
\(e_{T} \:\bot\:e_{X}\), \(e_{T} \:\bot\:e_{Y}\), and
\(e_{X} \:\bot\:e_{Y}\). Additional, the figures show one fork,
\(X \rightarrow \{T, Y\}\), and two colliders:
\(\{X, e_{T}\} \rightarrow T\) and \(\{X, T, e_{Y}\} \rightarrow Y\).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X & := f_{X}(e_{X}) \\
  T & := f_{T}(X,e_{T}) \\
  Y & := f_{Y}(T,X,e_{Y}) \\
  e_{T} & \:\bot\:e_{X} \\
  e_{T} & \:\bot\:e_{Y} \\
  e_{X} & \:\bot\:e_{Y}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_example1}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_example1.png}

}

\subcaption{\label{fig-mdag_example1}DAG}

\end{minipage}%

\caption{\label{fig-example1}Plausible causal structure the scenario
outlined in Section~\ref{sec-appendixB2}.}

\end{figure}%

\paragraph{The probabilistic implications of these
blocks}\label{sec-appendixB32}

Beyond their graphical capabilities, SCMs and DAGs can encode the
probabilistic information embedded within a causal structure. They
achieve this encoding by relying on three fundamental assumptions: the
local Markov, the minimality, the causal edges assumption. The
\emph{local Markov assumption} encodes probabilistic independencies
between variables by declaring that nodes in a graph are independent of
all its non-descendants, given its parents \citep[pp.~20]{Neal_2020}.
Meanwhile, the \emph{minimality assumption} encodes probabilistic
dependencies among variables by stating that every pair of adjacent
nodes exhibits a dependency \citep[pp.~21]{Neal_2020}. Finally, the
\emph{causal edges assumption} encodes causal relationships between
variables by declaring that each parent node acts as a direct cause of
its children \citep[pp.~22]{Neal_2020}. Figure~\ref{fig-ACflow}
illustrates how these assumptions influence the statistical and causal
interpretations of graphs.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/png/ACflow.png}

}

\caption{\label{fig-ACflow}The flow of association and causation in
graphs. Extracted and slightly modified from \citet[pp.~31]{Neal_2020}}

\end{figure}%

A notable implication of the assumptions underlying the probabilistic
encoding is that any conceptual model described by an SCM and DAG can
represent the joint distribution of variables more efficiently
\citep[pp.~29]{Pearl_et_al_2016}. This expression takes the form of a
product of conditional probability distributions (CPDs) of the type
\(P(child \:|\:parents)\). This property is formally known as the
\emph{Bayesian Network factorization} (BNF,
Equation~\ref{eq-net_factor})
\citetext{\citealp[pp.~29]{Pearl_et_al_2016}; \citealp[pp.~21]{Neal_2020}}.
In this expression, \(pa(X_{i})\) denotes the set of variables that are
the parents of \(X_{i}\).

\begin{equation}\phantomsection\label{eq-net_factor}{
\begin{aligned}
P(X_{1}, X_{2}, \dots, X_{P}) & = X_{1} \cdot \prod^{P}_{p=2} P(X_{i} \:|\:X_{i-1}, \dots, X_{1}) & (\small{\text{by chain rule})}\\
& = X_{1} \cdot \prod^{P}_{p=2} P(X_{i} \:|\:pa(X_{i}) ) & (\small{\text{by BNF}})
\end{aligned}
}\end{equation}

This encoding enables researchers with conceptual (theoretical)
knowledge in the form of an SCM and DAG to predict patterns of
(in)dependencies in the data. As highlighted by
\citet[pp.~35]{Pearl_et_al_2016}, these predictions depend solely on the
structure of these conceptual models without requiring the quantitative
details of the equations or the distributions of the errors. Moreover,
once researchers observe empirical data, the patterns of
(in)dependencies in the data can provide significant insights into the
validity of the proposed conceptual model.

The five fundamental building blocks described in
Section~\ref{sec-appendixB31} clearly illustrate which (in)dependencies
can SMCs and DAGs predict. For instance, applying the BNF to the causal
structure shown in the SCM \ref{fig-scm_bb1} and DAG \ref{fig-mdag_bb1}
enables researchers to express the joint probability distribution of the
observed variables as \(P(X_{1}, X_{3}) = P(X_{1}) P(X_{3})\),
supporting the theoretical assumption that the observed variables
\(X_{1}\) and \(X_{3}\) are unconditionally independent
(\(X_{1} \:\bot\:X_{3}\)) \citep[pp.~24]{Neal_2020}. Conversely, when
\(X_{3}\) is unconditionally dependent on \(X_{1}\)
(\(X_{1} \:\not\bot\:X_{3}\)), as depicted in the SCM \ref{fig-scm_bb2}
and DAG \ref{fig-mdag_bb2}, the BNF express their joint probability
distribution as \(P(X_{1}, X_{3}) = P(X_{3} \:|\:X_{1}) P(X_{1})\).
Notably, these descriptions demonstrate the clear correspondence between
the structural equations illustrated in Section~\ref{sec-appendixB31}
and the CPDs.

Beyond the insights gained from two-node structures, researchers can
uncover more nuanced patterns of(in)dependencies from chains, forks, and
colliders. These (in)dependencies apply to any data set generated by a
causal model with those structures, regardless of the specific functions
attached to the SCM \citep[pp.~36]{Pearl_et_al_2016}. For instance,
applying the BNF to the chain structure depicted in the SCM
\ref{fig-scm_bb3} and DAG \ref{fig-mdag_bb3} allow researchers to
represent the joint distribution for the observed variables as
\(P(X_{1},X_{2},X_{3}) =\)
\(P(X_{1}) P(X_{2} \:|\:X_{1}) P(X_{3} \:|\:X_{2})\). This expression
implies that \(X_{1}\) and \(X_{3}\) are unconditionally dependent
\((X_{1} \:\not\bot\:X_{3})\), but conditionally independent when
controlling for \(X_{2}\) \((X_{1} \:\bot\:X_{3} \:|\:X_{2})\).
Moreover, in the fork structure shown in the SCM \ref{fig-scm_bb4} and
DAG \ref{fig-mdag_bb4}, researchers can express the joint distribution
of the observed variables as \(P(X_{1},X_{2},X_{3}) =\)
\(P(X_{1} \:|\:X_{2}) P(X_{2}) P(X_{3} \:|\:X_{2})\). Similar to the
chain structure, this expression allows researchers to further infer
that \(X_{1}\) and \(X_{3}\) are unconditionally dependent
\((X_{1} \:\not\bot\:X_{3})\), but conditionally independent when
controlling for \(X_{2}\) \((X_{1} \:\bot\:X_{3} \:|\:X_{2})\). Finally,
researchers analyzing the collider structure illustrated in the SCM
\ref{fig-scm_bb5} and DAG \ref{fig-mdag_bb5} can express the joint
distribution of the observed variables as \(P(X_{1},X_{2},X_{3}) =\)
\(P(X_{1}) P(X_{2} \:|\:X_{1}, X_{3}) P(X_{3})\). This representation
allows researchers to infer that \(X_{1}\) and \(X_{3}\) are
unconditionally independent \((X_{1} \:\bot\:X_{3})\), but conditionally
dependent when controlling for \(X_{2}\)
\((X_{1} \:\not\bot\:X_{3} \:|\:X_{2})\). The authors \citet[pp.~37, 40,
41]{Pearl_et_al_2016} and \citet[pp.~25--26]{Neal_2020} provide the
mathematical proofs for these conclusions.

Using these additional probabilistic insights, researchers can revisit
the scenario in Section~\ref{sec-appendixB2}. In this context, applying
the BNF to the SCM \ref{fig-scm_example2} structure, enables the
representation of the joint probability distribution of the observed
variables as \(P(Y, T, X) =\) \(P(Y \:|\:T, X) P(T \:|\:X) P(X)\). From
this expression, researchers can infer that the outcome \(Y\) is
unconditionally dependent on the teaching method \(T\)
\((Y \:\not\bot\:T)\). This dependency arises from two key structures: a
direct causal path from the teaching method \(T\) to the outcome \(Y\),
represented by the two-connected-nodes structure \(T \rightarrow Y\)
(black path in DAG \ref{fig-mdag_example2}), and a confounding
non-causal path from the teaching method \(T\) to the outcome \(Y\)
through the socio-economic status of the school \(X\), represented by
the fork structure \(T \leftarrow X \rightarrow Y\) (gray path in DAG
\ref{fig-mdag_example2}).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X & := x \\
  T & := f_{T}(x,e_{T}) \\
  Y & := f_{Y}(T,x,e_{Y}) \\
  e_{T} & \:\bot\:e_{X} \\
  e_{T} & \:\bot\:e_{Y} \\
  e_{X} & \:\bot\:e_{Y}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_example2}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_example1a.png}

}

\subcaption{\label{fig-mdag_example2}Conditioned DAG}

\end{minipage}%

\caption{\label{fig-example2}Plausible causal structure the scenario
outlined in Section~\ref{sec-appendixB2}.}

\end{figure}%

\paragraph{From probability to causality}\label{sec-appendixB33}

The structural approach to causal inference translates probabilistic
insights into actionable strategies seeking to identify the ACE from
associational quantities. The approach achieves this by relying on the
\emph{modularity assumption}, which posits that intervening on a node
alters only the causal mechanism of that node, leaving others unchanged
\citep[pp.~34]{Neal_2020}.

The modularity assumption underpins the concepts of manipulated graphs
and Truncated Factorization, which are essential for representing
interventions \(P(Y_{i} \:|\:do(T_{i}=t))\) within SCMs and DAGs.
\emph{Manipulated graphs} simulate physical interventions by removing
specific edges from a DAG, while preserving the remaining structure
unchanged \citep[pp.~34]{Neal_2020}. In parallel, \emph{Truncated
Factorization} (TF) achieves a similar simulation by removing specific
functions from the conceptual model and replacing them with constants,
while keeping the rest of the structure unchanged \citep{Pearl_2010}.
The probabilistic implications of this factorization are formalized in
Equation~\ref{eq-trunc_factor}, where \(S\) represents the subset of
variables \(X_{p}\) directly influenced by the intervention, while an
example illustrating these concepts follows below.

\begin{equation}\phantomsection\label{eq-trunc_factor}{
P(X_{1}, X_{2}, \dots, X_{P} \:|\:do(S)) =
\begin{cases}
  \prod P(X_{p} \:|\:pa(X_{p}) ) & \text{if} \: p \not\in S \\
  1 \quad & \text{otherwise}
\end{cases}
}\end{equation}

Using the TF, researchers can define the \emph{backdoor adjustment} to
identify the ACE. This adjustment states that if a variable
\(X_{p} \in S\) serves as a \emph{sufficient adjustment set} for the
effect of \(X_{a}\) on \(X_{b}\), then the ACE can be identified using
Equation~\ref{eq-backdoor}. The sufficient adjustment set (potentially
empty) must block all non-causal paths between \(X_{a}\) and \(X_{b}\)
without introducing new paths. If such a set exists, then \(X_{a}\) and
\(X_{b}\) are \emph{d-separated} by \(X_{p}\)
(\(X_{a} \:\bot\:X_{b} \:|\:X_{p}\)) \citep{Pearl_2009}, and \(X_{p}\)
satisfies the \emph{backdoor criterion} \citep[pp.~37]{Neal_2020}.

\begin{equation}\phantomsection\label{eq-backdoor}{
P(X_{a} \:|\:do(X_{b}=x)) = \sum_{Xp} P(X_{a} \:|\:X_{b}=x, X_{p}) P(X_{p})
}\end{equation}

Ultimately, the backdoor adjustment enables researchers to express the
ACE as:

\begin{equation}\phantomsection\label{eq-backdoor_adjustment}{
\begin{aligned}
\tau & = E[X_{a} \:|\:do(X_{b}=1)]- E[X_{a} \:|\:do(X_{b}=2)] \\
  & = E_{Xp}\left[ E[X_{a} \:|\:do(X_{b}=1), X_{p}]- E[X_{a} \:|\:do(X_{b}=2), X_{p}] \right] \\
  & = \sum_{Xp} X_{a} \cdot P(X_{a} \:|\:X_{b}=1, X_{p}) \cdot P(X_{p}) - \sum_{Xp} X_{a} \cdot P(X_{a} \:|\:X_{b}=2, X_{p}) \cdot P(X_{p})
\end{aligned}
}\end{equation}

With these new insights, researchers revisiting the scenario in
Section~\ref{sec-appendixB32} can infer that the socio-economic status
of the school, \(X\), satisfies the backdoor criterion, assuming the
causal structure depicted by the SCM \ref{fig-scm_example2} and DAG
\ref{fig-mdag_example2} is correct. This means that \(X\) serves as a
sufficient adjustment set, as it effectively blocks all confounding
non-causal paths introduced by the fork structure. Nevertheless, since
\(Y\) remains dependent on \(T\) even after conditioning
\((Y \:\not\bot\:T \:|\:X)\), this dependency can only be attributed to
the direct causal effect \(T \rightarrow Y\). Notably, for the purpose
of identification, the conditioned DAG \ref{fig-mdag_example2} is
equivalent to the manipulated DAG \ref{fig-mdag_example3}, because \(X\)
satisfies the backdoor criterion.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\[
\begin{aligned}
  X & := f_{X}(e_{X}) \\
  T & := t \\
  Y & := f_{Y}(t,X,e_{Y}) \\
  e_{T} & \:\bot\:e_{X} \\
  e_{T} & \:\bot\:e_{Y} \\
  e_{X} & \:\bot\:e_{Y}
\end{aligned}
\]

}

\subcaption{\label{fig-scm_example3}SCM}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/png/mdag_example1b.png}

}

\subcaption{\label{fig-mdag_example3}Manipulated DAG}

\end{minipage}%

\caption{\label{fig-example3}Plausible causal structure the scenario
outlined in Section~\ref{sec-appendixB32}.}

\end{figure}%

Researchers can then apply the \emph{backdoor adjustment} to identify
the ACE of \(T\) on \(Y\). They achieve this by first identifying the
CACE of \(T\) on \(Y\) by conditioning on \(X\), and then marginalizing
this effect over \(X\) to obtain the ACE. This process is expressed in
Equation~\ref{eq-mACE2} (see Section~\ref{sec-appendixB2}).

\begin{equation}\phantomsection\label{eq-mACE2}{
\begin{aligned}
  \tau & = E[Y_{i} \:|\:do(T_{i}=1)]- E[Y_{i} \:|\:do(T_{i}=2)] \\
  & = E_{X}\left[ E[Y_{i} \:|\:T_{i}=1, X] - E[Y_{i} \:|\:T_{i}=2, X] \right] \\
  & = \sum_{X} Y_{i} \cdot P( Y_{i} \:|\:T_{i}=1, X) \cdot P(X) - \sum_{X} Y_{i} \cdot P( Y_{i} \:|\:T_{i}=2, X) \cdot P(X)
\end{aligned}
}\end{equation}

\paragraph{The estimation process}\label{sec-appendixB34}

Ultimately, researchers can use Bayesian inference methods to estimate
the ACE. The approach begins by defining two probability distributions:
the likelihood of the data,
\(P(X_{1}, X_{2}, \dots, X_{P} \:|\:\theta)\), and the prior
distribution, \(P(\theta)\) \citep{Everitt_et_al_2010}, where \(X_{P}\)
represents a random variable, and \(\theta\) represents a
one-dimensional parameter space for simplicity. After observing
empirical data, researchers can update the priors to posterior
distributions using Bayes' rule in Equation~\ref{eq-bayes_rule}:

\begin{equation}\phantomsection\label{eq-bayes_rule}{
P(\theta \:|\:X_{1}, X_{2}, \dots, X_{P}) = \frac{P(X_{1}, X_{2}, \dots, X_{P} \:|\:\theta) \cdot P(\theta)}{P(X_{1}, X_{2}, \dots, X_{P})}
}\end{equation}

Given that the denominator on the right-hand side of
Equation~\ref{eq-bayes_rule} serves as a normalizing constant
independent of the parameter \(\theta\), researchers can simplify the
posterior updating process into three steps. First, they integrate new
empirical data through the likelihood. Second, they update the
parameters' priors to a posterior distribution according to
Equation~\ref{eq-prop_rule}. Ultimately, they normalize these results to
obtain a valid probability distribution.

\begin{equation}\phantomsection\label{eq-prop_rule}{
P(\theta \:|\:X_{1}, X_{2}, \dots, X_{P}) \propto P(X_{1}, X_{2}, \dots, X_{P}\:|\:\theta) \cdot P(\theta)
}\end{equation}

Temporarily setting aside the definition of prior distributions
\(P(\theta)\), note that the posterior updating process depends heavily
on the assumptions underlying the likelihood of the data. However, as
the number of random variables, \(P\), increases, this joint
distribution quickly becomes intractable \citep{Neal_2020}. This
intractability is evident from Equation~\ref{eq-like_chain}, where the
likelihood distribution is expressed by multiple chained CPDs.

\begin{equation}\phantomsection\label{eq-like_chain}{
P(X_{1}, X_{2}, \dots, X_{P} \:|\:\theta) = P(X_{1} \:|\:\theta) \prod^{P}_{p=2} P(X_{i} \:|\:X_{i-1}, \dots, X_{1}, \theta )
}\end{equation}

Nevertheless, researchers can manage the complexity of the likelihood by
assuming specific local (in)dependencies among variables. SCMs and DAGs
provide a formal framework to represent these assumptions, as detailed
in Section~\ref{sec-appendixB32}. These assumptions improve model
tractability and simplify the estimation process by enabling the
derivation of the BNF of the likelihood (Equation~\ref{eq-like_BNF}).
With this simplified structure, any probabilistic programming language
can model the system and compute the parameter's posterior distribution
using Equation~\ref{eq-bayes_rule}.

\begin{equation}\phantomsection\label{eq-like_BNF}{
P(X_{1}, X_{2}, \dots, X_{P} \:|\:\theta) = P(X_{1} \:|\:\theta) \prod^{P}_{p=2} P(X_{i} \:|\:pa(X_{i}), \theta )
}\end{equation}

\newpage{}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{references.bib}





\end{document}
