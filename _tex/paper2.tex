% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  authoryear,
  review,
  1p]{elsarticle}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{elsarticle-harv}


\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{Psychometrika}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Let's talk about Thurstone \& Co.: An information-theoretical model for comparative judgments, and its statistical translation},
  pdfauthor={Jose Manuel Rivera Espejo; Tine van van Daal; Sven De De Maeyer; Steven Gillis},
  pdfkeywords={causal inference, directed acyclic graphs, structural
causal models, bayesian statistical methods, thurstonian
model, comparative judgement, probability, statistical modeling},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{Let's talk about Thurstone \& Co.: An information-theoretical
model for comparative judgments, and its statistical translation}
\author[1]{Jose Manuel Rivera Espejo%
\corref{cor1}%
}
 \ead{JoseManuel.RiveraEspejo@uantwerpen.be} 
\author[1]{Tine van Daal%
%
}
 \ead{tine.vandaal@uantwerpen.be} 
\author[1]{Sven De Maeyer%
%
}
 \ead{sven.demaeyer@uantwerpen.be} 
\author[2]{Steven Gillis%
%
}
 \ead{steven.gillis@uantwerpen.be} 

\affiliation[1]{organization={University of Antwerp, Training and
education sciences},,postcodesep={}}
\affiliation[2]{organization={University of
Antwerp, Linguistics},,postcodesep={}}

\cortext[cor1]{Corresponding author}




        
\begin{abstract}
This study revisits Thurstone's law of comparative judgment (CJ),
focusing on two prominent issues of traditional approaches. First, it
critiques the heavy reliance on Thurstone's Case V assumptions and, by
extension, the Bradley-Terry-Luce (BTL) model when analyzing CJ data.
Specifically, the study raises concerns about the assumptions of equal
discriminal dispersions and zero correlation between the stimuli. While
these assumptions simplify the trait measurement model, they may fail to
capture the complexity of CJ data, potentially leading to unreliable and
inaccurate trait estimates. Second, the study highlights the apparent
disconnect between CJ's trait measurement and hypothesis testing
processes. Although separating these processes simplifies the analysis
of CJ data, it may also undermine the reliability of various statistical
results derived from these processes.

To address these issues, the study extends Thurstone's general form
using a systematic and integrated approach based on Causal and Bayesian
inference methods. This extension integrates core theoretical principles
alongside key CJ assessment design features, such as the selection of
judges, stimuli, and comparisons. It then translates these elements into
a probabilistic statistical model for analyzing dichotomous CJ data,
overcoming the rigid assumptions of Case V and the BTL model.

Finally, the study emphasizes the relevance of this extension for
contemporary empirical CJ research, particularly stressing the need for
bespoke CJ models tailored to the experiments and data assumptions. It
also lays the foundation for broader applications, encouraging
researchers across the social sciences to adopt more robust and
interpretable methodologies.
\end{abstract}





\begin{keyword}
    causal inference \sep directed acyclic graphs \sep structural causal
models \sep bayesian statistical methods \sep thurstonian
model \sep comparative judgement \sep probability \sep 
    statistical modeling
\end{keyword}
\end{frontmatter}
    

\newcommand{\dsep}{\:\bot\:}
\newcommand{\ndsep}{\:\not\bot\:}

\section{Introduction}\label{sec-introduction}

In \emph{comparative judgment} (CJ) studies, judges assess a specific
trait or attribute across different stimuli by performing pairwise
comparisons \citep{Thurstone_1927a, Thurstone_1927b}. Each comparison
produces a dichotomous outcome, indicating which stimulus is perceived
to have a higher trait level. For example, when assessing writing
quality, judges compare pairs of written texts (the stimuli) to
determine the relative writing quality each text exhibit (the trait)
\citep{Laming_2004, Pollitt_2012b, Whitehouse_2012, vanDaal_et_al_2016, Lesterhuis_2018_thesis, Coertjens_et_al_2017, Goossens_et_al_2018, Bouwer_et_al_2023}.

Numerous studies have documented the effectiveness of CJ in assessing
traits and competencies over the past decade. These studies have
highlighted three aspects of the method's effectiveness: its
reliability, validity, and practical applicability. Research on
reliability suggests that CJ requires a relatively modest number of
pairwise comparisons \citep{Verhavert_et_al_2019, Crompvoets_et_al_2022}
to generate trait scores that are as precise and consistent as those
generated by other assessment methods
\citep{Coertjens_et_al_2017, Goossens_et_al_2018, Bouwer_et_al_2023}. In
addition, the evidence suggests that the reliability and time efficiency
of CJ are comparable, if not superior, to those of other assessment
methods when employing adaptive comparison algorithms
\citep{Pollitt_2012b, Verhavert_et_al_2022, Mikhailiuk_et_al_2021}.
Meanwhile, research on validity indicates the capacity of CJ scores to
represent accurately the traits under measurement
\citep{Whitehouse_2012, vanDaal_et_al_2016, Lesterhuis_2018_thesis, Bartholomew_et_al_2018, Bouwer_et_al_2023}.
Lastly, research on its practical applicability highlights CJ's
versatility across both educational and non-educational contexts
\citep{Kimbell_2012, Jones_et_al_2015, Bartholomew_et_al_2018, Jones_et_al_2019, Marshall_et_al_2020, Bartholomew_et_al_2020b, Boonen_et_al_2020}.

Nevertheless, despite the increasing number of CJ studies, research in
this domain remains unsystematic and fragmented, leaving several
critical issues unresolved. This study identifies and discusses two
prominent issues of traditional approaches that can undermine the
reliability and validity of CJ's trait estimates. First, it critiques
the heavy reliance on Thurstone's Case V assumptions
\citep{Thurstone_1927b} and, by extension, the Bradley-Terry-Luce (BTL)
model \citep{Bradley_et_al_1952, Luce_1959} when analyzing CJ data.
Specifically, the study raises concerns about the assumptions of equal
discriminal dispersions and zero correlation between the stimuli. While
these assumptions simplify the trait measurement model, they may fail to
capture the complexity of some traits or account for heterogeneous
stimuli, potentially leading to unreliable and inaccurate trait
estimates. Second, the study highlights the disconnect between CJ's
trait measurement and hypothesis testing processes. Although separating
these processes simplifies the analysis of CJ data, it may also
undermine the reliability of various statistical inferences derived from
these processes.

To address these issues, this study extends Thurstone's general form
through a systematic and integrated approach that combines causal and
Bayesian inference methods. In addition to potentially enhancing
measurement reliability and validity, and improving statistical accuracy
in hypothesis testing, this approach offers two key advantages. First,
it clarifies the interactions among all actors and processes involved in
CJ assessments. Second, it shifts the current comparative data analysis
paradigm from passively accepting Case V and the BTL model assumptions
to actively testing whether those assumptions fit the data under
analysis.

As a result, the study divides its content into six main sections.
Section~\ref{sec-thurstone_theory} provides an overview of Thurstone's
theory. Section~\ref{sec-theory-issues} discusses the identified issues
in detail. Section~\ref{sec-theoretical} extends Thurstone's general
form to address these challenges. The extension integrates core
theoretical principles alongside key CJ assessment design features, such
as the selection of judges, stimuli, and comparisons.
Section~\ref{sec-statistical} translates these theoretical and practical
elements into a probabilistic statistical model to analyze dichotomous
pairwise comparison data. Finally, Section~\ref{sec-discussion}
discusses the findings, explores avenues for future research, and detail
the challenges for future researchers.

\section{Thurstone's theory}\label{sec-thurstone_theory}

In its most general form, Thurstone's theory addresses pairwise
comparisons wherein a single judge evaluates multiple stimuli
\citep{Thurstone_1927b}. The theory posits that two key factors
determine the dichotomous outcome of these comparisons: the discriminal
process of each stimulus and their discriminal difference. The
\emph{discriminal process} captures the psychological impact each
stimulus exerts on the judge or, more simply, his perception of the
stimulus trait. The theory assumes that the discriminal process for any
given stimulus forms a Normal distribution along the trait continuum
\citep{Thurstone_1927b}. The mode (mean) of this distribution, known as
the \emph{modal discriminal process}, indicates the stimulus position on
this continuum, while its dispersion, referred to as the
\emph{discriminal dispersion}, reflects variability in the perceived
trait of the stimulus.

Figure~\ref{fig-discriminal_process} illustrates the hypothetical
discriminal processes along a quality trait continuum for two written
texts. The figure indicates that the modal discriminal process for Text
B is positioned further along the continuum than that of Text A
\((T_{B} > T_{A})\), suggesting that Text B exhibits higher quality.
Additionally, the figure highlights that Text B has a broader
distribution compared to Text A, which arises from its larger
discriminal dispersion \((\sigma_{B} > \sigma_{A})\).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/discriminal_process.png}

}

\subcaption{\label{fig-discriminal_process}Discriminal processes}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/discriminal_difference.png}

}

\subcaption{\label{fig-discriminal_difference}Discriminal difference}

\end{minipage}%

\caption{\label{fig-thurstone_theory}Hypothetical discriminal processes
and discriminant difference along a quality trait continuum for two
written texts.}

\end{figure}%

However, since the individual discriminal processes of the stimuli are
not directly observable, the theory introduces the \emph{law of
comparative judgment}. This law posits that in pairwise comparisons, a
judge perceives the stimulus with a discriminal process positioned
further along the trait continuum as possessing more of the trait
\citep{Bramley_2008}. This suggests that pairwise comparison outcomes
depend on the relative distance between stimuli, not their absolute
positions on the continuum. Indeed, the theory assumes that the
difference between the underlying discriminal processes of the stimuli,
referred to as the \emph{discriminal difference}, determines the
observed dichotomous outcome. Furthermore, the theory assumes that
because the individual discriminal processes form a Normal distribution
on the continuum, the discriminal difference will also conform to a
Normal distribution \citep{Andrich_1978}. In this distribution, the mode
(mean) represents the average relative separation between the stimuli,
and its dispersion indicates the variability of that separation.

Figure~\ref{fig-discriminal_difference} illustrates the distribution of
the discriminal difference for the two hypothetical texts. The figure
indicates that the judge perceives Text B as having significantly higher
quality than Text A. Two key observations support this conclusion: the
positive difference between their modal discriminal processes
\((T_{B} - T_{A} > 0)\) and the probability area where the discriminal
difference distinctly favors Text B over Text A, represented by the
shaded gray area denoted as \(P(B > A)\). As a result, the dichotomous
outcome of this comparison is more likely to favor Text B over A.

\section{Two Prominent Issues in Traditional CJ
Practice}\label{sec-theory-issues}

Thurstone noted from the outset that his general formulation, described
in Section~\ref{sec-thurstone_theory}, led to a \emph{trait scaling
problem}. Specifically, the model required estimating more ``unknown''
parameters than the number of available pairwise comparisons
\citep{Thurstone_1927b}. For instance, in a CJ assessment with five
texts, the general form would require estimating \(20\) parameters: five
modal discriminal processes, five discriminal dispersions, and \(10\)
correlations--one per comparison (see Table~\ref{tbl-thurstone_cases}).
However, a single judge could only provide \({5 \choose 2} = 10\) unique
comparisons, an insufficient data set to estimate the required
parameters.

To address this issue and facilitate the practical implementation of the
theory, Thurstone developed five cases derived from this general form,
each progressively incorporating additional simplifying assumptions
\citep{Thurstone_1927b}. In Case I, Thurstone postulated that pairs of
stimuli would maintain a constant correlation across all comparisons. In
Case II, he allowed multiple judges to undertake comparisons instead of
confining evaluations to a single judge. In Case III, he posited that
there was no correlation between stimuli. In Case IV, he assumed that
the stimuli exhibited similar dispersions. Finally, in Case V, he
replaced this assumption with the condition that stimuli had equal
discriminal dispersions. Table~\ref{tbl-thurstone_cases} summarizes the
assumptions of the general form and the five cases. For a detailed
discussion of these cases and their progression, refer to
\citet{Thurstone_1927b} and \citet[pp.~248--253]{Bramley_2008}.

\begin{table}

\caption{\label{tbl-thurstone_cases}Thurstones cases and their
asumptions}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/thurstone_cases.png}

}

\end{table}%

However, Thurstone developed Case V prioritizing statistical simplicity
over precise trait measurement and offering no guidance on how to use
its trait estimates for statistical inference or hypothesis testing.
Specifically, Thurstone cautioned that its use ``should not be made
without (an) experimental test'' \citep[pp.~270]{Thurstone_1927b}, as it
imposes the most extensive set of simplifying assumptions
\citep{Bramley_2008, Kelly_et_al_2022} (see
Table~\ref{tbl-thurstone_cases}). Moreover, because Thurstone's primary
goal was to produce a ``rather coarse scaling'' of traits and ``allocate
the compared stimuli on this continuum''
\citep[pp.~269]{Thurstone_1927b}, his theory did not support formal
statistical inference. Despite these limitations, it is surprising that
CJ research has predominantly relied on Case V to measure different
traits, which raises significant concerns about the reliability and
validity of such measurements in contexts where the case's assumptions
may not hold \citep{Kelly_et_al_2022, Andrich_1978}. Furthermore,
although the CJ tradition has attempted to address the gap of hypothesis
testing by using the point estimates of the traits--or their
transformations--a critical question remains: Does this approach provide
a valid foundation for statistical inference?

Thus, this section discusses these two prominent issues. Specifically,
Section~\ref{sec-theory-issue1} examines the heavy reliance on
Thurstone's Case V assumptions in the statistical analysis of CJ data.
Conversely, Section~\ref{sec-theory-issue2} focuses on the apparent
disconnect between the approaches to trait measurement and hypothesis
testing in CJ.

\subsection{The Case V and the statistical analysis of CJ
data}\label{sec-theory-issue1}

As previously discussed, Case V remains the most widely used model in CJ
literature. This preference largely stems from the widespread adoption
of the BTL model, which provides a simplified statistical representation
of the case. The BTL model mirrors most of Case V's assumptions, with
one notable distinction. While Case V assumes a Normal distribution for
the stimuli' discriminal processes, the BTL model uses the more
mathematically tractable Logistic distribution
\citep{Andrich_1978, Bramley_2008} (see
Table~\ref{tbl-thurstone_cases}). However, this substitution has minimal
impact on trait estimation or model interpretation because the scale of
the discriminal process (i.e., the latent trait) is arbitrary up to a
non-monotonic transformation
\citep{vanderLinden_et_al_2017_I, McElreath_2021}. That is, as long as
the substitution (transformation) preserves the data rank order, the
choice of distribution for the discriminal processes is inconsequential.
This condition is satisfied in this case, as the Normal and Logistic
distributions exhibit analogous statistical properties, differing only
by a scaling factor of approximately \(1.7\)
\citep{vanderLinden_et_al_2017_I}.

However, Thurstone acknowledged that some assumptions of Case V could be
problematic when researchers assess complex traits or heterogeneous
stimuli \citep{Thurstone_1927a}. Thus, given that modern CJ applications
often involve such traits and stimuli, two key assumptions of Case V,
and by extension, the BTL model, may not always hold in theory or
practice. These assumptions are the equal dispersion and zero
correlation between stimuli.

\subsubsection{The assumption of equal dispersions between
stimuli}\label{sec-theory-issue1a}

According to the theory, discrepancies in the discriminal dispersions of
stimuli shape the distribution of the discriminal difference, directly
influencing the outcome of pairwise comparisons. A thought experiment
can help illustrate this idea. In it, researchers observe the
discriminal processes for two texts, A and B, assuming that the
dispersion for Text A remains constant and that the two texts are
uncorrelated \((\rho=0)\). Figure~\ref{fig-dispersion} demonstrates that
an increase in the uncertainty associated with the perception of Text B
relative to Text A \((\sigma_{B} - \sigma_{A})\), broadens the
distribution of their discriminal difference. This broadening affects
the probability area where the discriminal difference distinctly favors
Text B over Text A, expressed as \(P(B>A)\), ultimately influencing the
comparison outcome. Additionally, the figure reveals that when the
discriminal dispersions of the texts are equal, as in the BTL model
\((\sigma_{B} - \sigma_{A}=0)\), the discriminal difference distribution
is more narrow than when the dispersions differ. As a result, the
discriminal difference is more likely to favor Text B over Text A, as it
is represented by the shaded gray area.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/dispersion.png}

}

\subcaption{\label{fig-dispersion}Discriminal Difference distribution
under varying discrepancies in stimuli dispersions}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/correlation.png}

}

\subcaption{\label{fig-correlation}Discriminal Difference distribution
under varying levels of correlation between stimuli}

\end{minipage}%

\caption{\label{fig-casev_issues}The effect of dispersion discrepancies
and stimuli correlation on the distribution of the discriminal
difference.}

\end{figure}%

In experimental practice, however, the thought experiment occurs in
reverse. Researchers first observe the comparison outcome and then use
the BTL model to infer the discriminal difference between stimuli and
their respective discriminal processes \citep{Thurstone_1927a}.
Consequently, the outcome's ability to reflect \emph{true} differences
between stimuli largely depends on the validity of the model's
assumptions \citep{Kohler_et_al_2019}, in this case, the assumption of
equal dispersions. For instance, when the assumption accurately captures
the complexity of the data, the BTL model estimates a discriminal
difference distribution that accurately represents the \emph{true}
discriminal difference between the texts. This scenario is illustrated
in Figure~\ref{fig-dispersion}, when the model's discriminal difference
distribution aligns with the \emph{true} discriminal difference
distribution, represented by the thick continuous line corresponding to
\(\sigma_{B}-\sigma_{A}=0\). The accuracy of this discriminal difference
then ensures reliable estimates for the texts' discriminal processes.

Notably, while assuming equal dispersions simplifies the trait
measurement model, evidence from the CJ literature suggests that this
assumption may fail accommodate for heterogeneous stimuli, such as
handwritten texts or English compositions
\citep{Thurstone_1927a, Andrich_1978, Bramley_2008, Kelly_et_al_2022}.
Indeed, the presence of the so-called \emph{misfit texts}--texts that
elicit more judgment discrepancies than others
\citep{Pollitt_2004, Pollitt_2012b, Pollitt_2012a, Goossens_et_al_2018}--may
signal this limitation, as such discrepancies can arise from larger
discriminal dispersions due to stimulus heterogeneity or because the
texts are genuine outliers--that is, texts with distinctive
characteristics that deviate markedly from the rest of the sample
\citep{Grubbs_1969}. In either case, the BTL model's assumptions prevent
it from adequately accounting for or addressing these anomalies, leaving
exclusion of the ``problematic'' texts as the primary remedy.

Significant statistical and measurement issues can arise when the
assumption of equal dispersions between stimuli does not hold.
Specifically, the BTL model may overestimate the trait's reliability,
that is, the degree to which the outcome accurately reflects the
\emph{true} discriminal differences between stimuli. This
overestimation, in turn, results in spurious conclusions about these
differences \citep{McElreath_2020, Wu_et_al_2022} and, by extension,
about the underlying discriminal processes of stimuli.
Figure~\ref{fig-dispersion} also illustrates this scenario when the
model's discriminal difference distribution aligns with the thick
continuous line for \(\sigma_{B}-\sigma_{A}=0\), while the \emph{true}
discriminal difference follows any discontinuous line where
\(\sigma_{B}-\sigma_{A} \neq 0\). Furthermore, if researchers
acknowledge that \emph{misfit statistics} may represent texts with
different dispersions or outlying observations, the common CJ practice
of excluding stimuli based on these statistics may unintentionally
discard valuable information \citep{Miller_2023}, introducing bias into
the trait estimates \citep{Zimmerman_1994, McElreath_2020}. The
direction and magnitude of these biases remain unpredictable, as they
depend on which stimuli researchers exclude from the analysis.

\subsubsection{The assumption of zero correlation between
stimuli}\label{sec-theory-issue1b}

The correlation \(\rho\), illustrated in
Figure~\ref{fig-discriminal_difference}, measures how much the judges'
perception of a specific trait in one stimulus depends on their
perception of the same trait in another. Similar to the discriminal
dispersions, this correlation shapes the distribution of the discriminal
difference, directly impacting the outcomes of pairwise comparisons. A
thought experiment, akin to the one presented in
Section~\ref{sec-theory-issue1a}, can illustrate this idea. Assuming
that the discriminal dispersions for both texts remain constant,
Figure~\ref{fig-correlation} shows that as the correlation between the
two texts increases, the distribution of their discriminal difference
becomes narrower. This narrowing, in turn, affects the probability that
the discriminal difference distinctly favors Text B over Text A--denoted
as \(P(B > A)\)--and thus directly influences the comparison outcome.
Furthermore, the figure shows that when two texts are independent or
uncorrelated, as assumed in the BTL model \((\rho=0)\), the distribution
of their discriminal difference is less narrow than in scenarios where
the texts are positively correlated. As a result, it becomes less likely
for the comparison to favor Text B over Text A, as indicated by the
larger shaded area.

Despite these notable differences in the distribution of the discriminal
difference under various correlational assumptions, in practice,
assessment designs often adopt the assumption of no correlation between
stimuli based on an old theoretical justification. Specifically,
\citet{Thurstone_1927b} argued that stimuli could be treated as
uncorrelated because judges' biases--arising from two opposing and
equally weighted effects occurring during the pairwise
comparisons--would cancel each other out. This idea was later formalized
by \citet{Andrich_1978}, who provided a mathematical demonstration of
this cancellation using the BTL model under the assumption of
discriminal processes with additive biases. However, evidence from the
CJ literature indicates that the assumption of zero correlation does not
hold in practice in at least two scenarios: when intricate aspects of
multidimensional, complex traits or heterogeneous stimuli influence
judges' perceptions or when additional hierarchical structures are
relevant to the stimuli.

In the first scenario, research on text quality suggests that when
judges evaluate complex, multidimensional traits or heterogeneous
stimuli, they often rely on a variety of intricate stimulus
characteristics to inform their judgments
\citep{vanDaal_et_al_2016, Lesterhuis_et_al_2018, Chambers_et_al_2022}.
Regardless of their relevance, these characteristics may not receive
equal weight or consistently oppose one another across comparisons. As a
result, they may exert a disproportionate influence on judges'
perceptions, generating biases that persist rather than cancel out. For
example, this could occur when a judge assessing the argumentative
quality of a text may place disproportionate emphasis on handwriting
clarity, thereby favoring neatly written texts despite their weaker
arguments. Moreover, because the discriminal process of stimuli becomes
an observable outcome only through the judges' perceptions, these biases
could introduce dependencies between the stimuli
\citep{vanderLinden_et_al_2017_II}. While direct evidence for this exact
scenario is limited, existing studies document the presence of judge
bias in CJ \citep[\citet{vanDaal_et_al_2016},
\citet{Bartholomew_et_al_2020a}]{Pollitt_et_al_2003}, reinforcing the
argument that the factors influencing pairwise comparisons do not always
cancel each other out.

In the second scenario, the shared context or inherent connections
introduced by additional hierarchical structures may create dependencies
between stimuli--a statistical phenomenon known as clustering
\citep{Everitt_et_al_2010}. For instance, when the same individual
produces multiple texts, those texts often exhibit common features, such
as writing style or overall quality, that judges can easily recognize.
In this regard, although the CJ literature acknowledges the existence of
such hierarchical structures, the statistical approaches to account for
this additional source of dependence have been insufficient. For
instance, when CJ data incorporates multiple samples of stimuli from the
same individuals, researchers frequently rely on (averaged) point
estimates of the BTL scores to conduct subsequent analyses and tests at
the individual level
\citep{Bramley_et_al_2019, Boonen_et_al_2020, Bouwer_et_al_2023, vanDaal_et_al_2017, Jones_et_al_2019, Gijsen_et_al_2021}.
However, this approach can introduce additional statistical and
measurement issues, similar to the ones discussed in
Section~\ref{sec-theory-issue2}.

Thus, erroneously assuming zero correlation between stimuli can also
lead to significant statistical and measurement issues. Specifically,
neglecting judges' biases or relevant hierarchical structures can create
dimensional mismatches in the model, leading to the over- or
underestimation of trait reliability
\citep{Ackerman_1989, Hoyle_et_al_2023} and even biases
\citep{Wu_et_al_2022}. This issue is illustrated in
Figure~\ref{fig-correlation} when the discriminal difference
distribution of the BTL scores follows the thick continuous line
\((\rho = 0)\), while the \emph{true} discriminal difference follows any
discontinuous line where \(\rho \neq 0\). Moreover, these inaccuracies
can result in spurious conclusions about the discriminal differences
\citep{McElreath_2020} and, by extension, the underlying discriminal
processes of the stimuli--potentially causing the model to incorrectly
classify stimuli or judges as \emph{misfits}.

Finally, similar to what we discussed in the previous section, removing
\emph{misfit} judges--that is, judges whose assessments deviate markedly
from the shared consensus
\citep{Pollitt_2012a, Pollitt_2012b, vanDaal_et_al_2016, Goossens_et_al_2018, Wu_et_al_2022},
and may appear as outliers under the BTL model
\citep{Wu_et_al_2022}--risk discarding valuable information and
introducing bias into trait estimates \citep{Miller_2023}. The direction
and magnitude of these biases remain unpredictable, as they depend on
which judges researchers exclude from the analysis
\citep{Zimmerman_1994, OHagan_2018, McElreath_2020}.

\subsection{The disconnect between trait measurement and hypothesis
testing}\label{sec-theory-issue2}

Building on the previous section, it is clear that, researchers in CJ
studies typically use the BTL model to measure traits and position the
compared stimuli along a latent continuum \citep{Thurstone_1927b}. The
CJ literature also show that they frequently rely on point estimates of
these traits--typically the BTL scores or its transformations--to
conduct statistical inference or hypothesis testing. For example,
researchers have used these scores to identify `misfit' judges and
stimuli \citep{Pollitt_2012b, vanDaal_et_al_2016, Goossens_et_al_2018},
detect biases in judges' ratings
\citep{Pollitt_et_al_2003, Pollitt_2012b}, calculate correlations with
other assessment methods \citep{Goossens_et_al_2018, Bouwer_et_al_2023},
or test hypotheses related to the underlying trait of interest
\citep{Casalicchio_et_al_2015, Bramley_et_al_2019, Boonen_et_al_2020, Bouwer_et_al_2023, vanDaal_et_al_2017, Jones_et_al_2019, Gijsen_et_al_2021}.

Nevertheless, while separating the trait measurement and hypothesis
testing processes simplifies the analysis of CJ data, the statistical
literature cautions against relying solely on the point estimates of BTL
scores to conduct statistical inference or hypothesis tests, as this
practice can undermine the resulting statistical conclusions. A key
consideration is that BTL scores are parameter estimates that inherently
carry uncertainty (measurement error). Ignoring this uncertainty can
bias the analysis and reduce the precision of hypothesis tests. The
direction and magnitude of such biases are often unpredictable. Results
may be attenuated, exaggerated, or remain unaffected depending on the
degree of uncertainty in the scores and the actual effects being tested
\citep{McElreath_2020, Kline_et_al_2023, Hoyle_et_al_2023}. Furthermore,
the reduced precision in hypothesis tests diminishes their statistical
power, increasing the likelihood of committing type-I or type-II errors
\citep{McElreath_2020}.

In aggregate, the heavy reliance on Thurstone's Case V assumptions in
the statistical analysis of comparative data can compromise the
reliability of trait estimates. This overreliance may also undermine
their validity \citep{Perron_et_al_2015}, particularly when coupled with
the disconnect between the trait measurement and hypothesis testing
processes. However, the structural approach to causal inference can
address these issues by offering a systematic and integrated framework
that strengthens measurement reliability and validity while enhancing
the statistical accuracy of hypothesis tests.

\section{Extending Thurstone's general form}\label{sec-theoretical}

The \emph{structural approach} to causal inference provides a formal
framework for identifying causes and estimating their effects using
data. The approach uses structural causal models (SCMs) and directed
acyclic graphs (DAGs)
\citep{Pearl_2009, Pearl_et_al_2016, Gross_et_al_2018, Neal_2020} to
formally and graphically represent the assumed causal structure of a
system, such as the one found in CJ assessments. Essentially, SCMs and
DAGs function as \emph{conceptual models} on which identification
analysis rests. \emph{Identification analysis} determines whether an
estimator can accurately compute an estimand based solely on its
(causal) assumptions, regardless of random variability
\citep{Schuessler_et_al_2023}. Here, \emph{estimands} represent the
specific quantities researchers aim to determine (i.e., a parameter)
\citep{Everitt_et_al_2010}. \emph{Estimators} denote the methods or
functions that transform data into an estimate (e.g., a statistical
model), while \emph{estimates} are the numerical values approximating
the estimand \citep{Neal_2020, Everitt_et_al_2010}.

A motivating example that will appear throughout this document clarifies
these concepts. In this example, researchers aim to answer the question:
``To what extent do different teaching methods influence students'
ability to produce high-quality written texts?'' To investigate this, a
researcher designs a CJ assessment by randomly assigning students
(individuals) to two groups, each receiving a different teaching method.
Judges then compare pairs of students' written texts (stimuli) to
produce a dichotomous outcome reflecting the relative quality of each
text (trait). Based on this setup, researchers can reformulate the
research question as the estimand: ``\emph{On average}, is there a
difference in the ability to produce high-quality written texts between
the two groups of students?''.

Following standard CJ practices, researchers typically use estimates
from the BTL model--or its transformations--to approximate this
estimand. However, as discussed in Section~\ref{sec-theory-issues},
Thurstone's Case V and the BTL model exhibit several statistical and
measurement limitations. These limitations hinder the model's ability to
identify various estimands relevant to CJ inquiries, including the one
described in the example.

Fortunately, SCMs and DAGs support identification analysis through two
key advantages\footnote{These topics are beyond the scope of this study,
  thus, readers seeking a more profound understanding can refer to
  introductory papers such as \citet{Pearl_2010}, \citet{Rohrer_2018},
  \citet{Pearl_2019}, and \citet{Cinelli_et_al_2020}, and introductory
  books like \citet{Pearl_et_al_2018}, \citet{Neal_2020}, and
  \citet{McElreath_2020} are useful. For more advanced study, seminal
  papers such as \citet{Neyman_et_al_1923}, \citet{Rubin_1974},
  \citet{Spirtes_et_al_1991}, and \citet{Sekhon_2009}, along with books
  such as \citet{Pearl_2009}, \citet{Morgan_et_al_2014}, and
  \citet{Hernan_et_al_2025}, are recommended.}. First, regardless of
complexity, they can represent various causal structures using only five
fundamental building blocks \citep{Neal_2020, McElreath_2024}. This
feature allows researchers to decompose complex structures into
manageable components, facilitating their analysis. Second, they depict
causal relationships in a non-parametric way. This flexibility enables
feasible identification strategies without requiring specification of
the types of variables, the functional forms relating them, or the
parameters of those functional forms \citep{Pearl_et_al_2016}.

Thus, this section addresses the issues identified in
Section~\ref{sec-theory-issues} by extending Thurstone's general form
using the structural approach to causal inference. Specifically, it
combines the core theoretical principles outlined in
Section~\ref{sec-thurstone_theory} with key CJ assessment design
features, such as the selection of judges, stimuli, and comparisons. In
addition to improving statistical accuracy and strengthening measurement
reliability and validity, the approach offers two key advantages. First,
it clarifies the interactions among all actors and processes involved in
CJ assessments. Second, it shifts the current comparative data analysis
paradigm from passively accepting the model assumptions to actively
testing whether those assumptions fit the data under analysis.

Accordingly, Section~\ref{sec-theory-theoretical_P} incorporates the
theoretical principles into what we refer to as the
\emph{conceptual-population model}. This model assumes an idealized
scenario where researchers have access to a \emph{conceptual population}
of comparative data, that is, data representing all repeated judgments
made by every available judge for each pair of stimuli produced by each
pair of individuals in the population. Conversely,
Section~\ref{sec-theory-theoretical_SC} integrates the assessment design
features into what we refer to as the \emph{sample-comparison model}.
This model assumes a more realistic scenario where researchers only have
access to a sample of judges, individuals, stimuli, and comparisons from
the conceptual population.

\subsection{The conceptual-population
model}\label{sec-theory-theoretical_P}

In the conceptual-population model, the idealized scenario of a
\emph{conceptual population} of comparative data enables the integration
of Thurstone's theoretical principles and provides a foundation for
proposing innovations aimed at addressing some of the issues discussed
in Section~\ref{sec-theory-issues}.

\subsubsection{Integrating the first theoretical
principles}\label{sec-theory-theoretical_P1}

Before incorporating the first theoretical principles of Thurstone's
theory, it is essential to further define SCMs. SCMs are formal
mathematical models characterized by a set of \emph{endogenous}
variables \(V\), a set of \emph{exogenous} variables \(E\), and a set of
functions \(F\)
\citep{Pearl_2009, Pearl_et_al_2016, Cinelli_et_al_2020}. Endogenous
variables are those whose causal mechanisms a researcher chooses to
model \citep{Neal_2020}. In contrast, exogenous variables represent
\emph{errors} or \emph{disturbances} arising from omitted factors that
the investigator chooses not to model explicitly \citep{Pearl_2009}.
Lastly, the functions, referred to as \emph{structural equations},
express the endogenous variables as non-parametric functions of other
endogenous and exogenous variables. These functions use the symbol
`\(:=\)' to denote the asymmetrical causal dependence between variables
and the symbol `\(\:\bot\:\)' to represent \emph{d-separation}, a
concept akin to statistical (conditional) independence.

SCM~\ref{fig-cj03_scm} presents the first theoretical principles
embedded in the conceptual-population model, which evaluates the impact
of different teaching methods on students' writing ability. This SCM
outlines the relationship between the conceptual-population outcome
\((O^{cp}_{iahbjk})\) and several related variables. The subscripts
\(i\) and \(h\) identify the students who authored the texts (i.e., the
individuals). The indices \(a\) and \(b\) represent the texts under
comparison (i.e., the stimuli). The index \(j\) indicates the judge
conducting the comparison, while the index \(k\) accounts for assessment
conditions where a judge compares the same pair of stimuli multiple
times, i.e., a \emph{repeated measures designs}
\citep[pp.~366-376]{Lawson_2015}. Thus, the indexing system supports
comparisons between different texts written by the same student
\((i = h;\) \(a \neq b)\) and between texts written by distinct students
\((i \neq h;\) where \(a = b\) is permitted\()\), each compared once or
repeatedly by all judges \((j = 1,\dots,n_{J};\) \(k = 1,\dots,n_K;\)
where \(n_{J}>1\) and \(n_{K}\geq1)\). However, it excludes cases where
a judge compares a student's text to itself, whether once or multiple
times \((i = h;\) \(a = b;\) \(j = 1,\dots,n_{J};\)
\(k = 1,\dots,n_{K};\) where \(n_{J}>1\) and \(n_{K}\geq1)\), as such
comparison lacks practical relevance within the CJ framework. Here,
\(n_{J}\) indicates the total number of judges, and \(n_{K}\) denotes
the number of repeated judgments each judge performs.

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{cp}_{iahbjk} & := f_{O}(D_{iahbjk}) \\
  D_{iahbjk} & := f_{D}(T_{ia}, T_{hb}, B_{jk})
\end{aligned}
\]

}

\subcaption{\label{fig-cj03_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.35\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_03.png}

}

\subcaption{\label{fig-cj03_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj03}Conceptual-population model, scalar form.}

\end{figure}%

In line with Thurstone's theory, SCM~\ref{fig-cj03_scm} depicts the
texts' discriminal processes \((T_{ia}, T_{hb})\) and their discriminal
difference \((D_{iahbjk})\) (see Section~\ref{sec-thurstone_theory}).
Additionally, the SCM incorporates a key CJ design feature: the judges'
biases \((B_{kj})\). This extension builds on the arguments presented in
Section~\ref{sec-theory-issue1b}, contending that the discriminal
difference becomes an observable outcome only through judges'
perceptions. Given that such perceptions may be imperfect--and that each
judge may carry some degree of bias
\citep[see][]{Pollitt_et_al_2003, vanDaal_et_al_2016}--it is reasonable
that judges' perceptions (bias) should be treated as an integral
component of the CJ system from the outset, as this leads to a more
accurate representation of the data-generating process underlying the
pairwise comparisons. This model defines the preliminary set of
endogenous variables,
\(V = \{ O_{iahbjk}, D_{iahbjk}, T_{ia}, T_{hb}, B_{kj} \}\), and the
preliminary set of structural equations, \(F = { f_{O}, f_{D} }\), which
capture the non-parametric dependencies among these variables.

Notably, every SCM has an associated DAG
\citep{Pearl_et_al_2016, Cinelli_et_al_2020}. A DAG is a \emph{graph}
consisting of nodes connected by edges, where nodes represent random
variables. The term \emph{directed} indicates that edges or arrows
extend from one node to another, indicating the direction of causal
influence. The absence of an edge implies no direct relationship between
the nodes. The term \emph{acyclic} means that the causal influences do
not form loops, ensuring the influences do not cycle back on themselves
\citep{McElreath_2020}. DAGs conventionally depict observed variables as
solid black circles and unobserved (latent) variables as open circles
\citep{Morgan_et_al_2014}. Although DAGs conventionally omit exogenous
variables for simplicity, the DAGs presented in this section includes
exogenous variables to improve clarity and reveal potential issues
related to conditioning and confounding \citep{Cinelli_et_al_2020}.

Figure~\ref{fig-cj03_dag} displays the DAG corresponding to
SCM~\ref{fig-cj03_scm}, illustrating the expected causal relationships
outlined in Thurstone's theory. The graph shows that the discriminal
processes of the texts \((T_{ia}, T_{hb})\) influence their discriminal
difference \((D_{iahbjk})\), which in turn determines the outcome
\((O^{cp}_{iahbjk})\). It also highlights the influence of judges'
biases \((B_{kj})\) on the discriminal difference. Additionally, the DAG
differentiates between observed endogenous variables, such as the
outcome (solid black circle), and latent endogenous variables, including
the texts' discriminal processes, their discriminal difference, and the
judges' biases (open circles).

\subsubsection{\texorpdfstring{The \emph{conceptual-population} data
structure}{The conceptual-population data structure}}\label{sec-theory-theoretical_P2}

Although specifying a data structure is not mandatory when using SCMs
and DAGs, defining one in this case can improve clarity and facilitate
the description of the system. Thus, to re-express the scalar form of
the CJ system shown in Figure~\ref{fig-cj03} into an equivalent
vectorized form, we first define the vectors \(I\) and \(J\), along with
the matrices \(IA\) and \(JK\), as in Equation (\ref{eq-mat11}). Here,
each element of \(I\) represents a unique individual \(i\) or \(h\),
where \(n_{I}\) denotes the total number of individuals. Similarly, each
element of \(J\) corresponds to a unique judge \(j\), with \(n_{J}\)
indicating the total number of judges. Moreover, each row of \(IA\)
represents a unique pairing of individuals \(i, h\) with stimuli
\(a, b\). As a result, the matrix \(IA\) contains \(n_{I} \cdot n_{A}\)
rows and \(2\) columns, where \(n_{A}\) specifies the number of stimuli
available per individual. Likewise, each row of \(JK\) associates a
judge \(j\) with a (repeated) judgment index \(k\). Consequently, the
matrix \(JK\) has \(n_{J} \cdot n_{K}\) rows and \(2\) columns, where
\(n_{K}\) indicates the number of repeated judgments each judge makes.
\begin{equation}\phantomsection\label{eq-mat11}{
I = \begin{bmatrix}
1 \\
\vdots \\
i \\
\vdots \\
h \\
\vdots \\
n_{I}
\end{bmatrix} ; \;
J = \begin{bmatrix}
1 \\
\vdots \\
j \\
\vdots \\
n_{J}
\end{bmatrix} ; \;
IA = \begin{bmatrix}
1 & 1 \\
\vdots & \vdots \\
1 & n_{A} \\
\vdots & \vdots \\
i & a \\
\vdots & \vdots \\
h & b \\
\vdots & \vdots \\
n_{I} & 1 \\
\vdots & \vdots \\
n_{I} & n_{A}
\end{bmatrix} ; \;
JK = \begin{bmatrix}
1 & 1 \\
\vdots & \vdots \\
1 & n_{K} \\
\vdots & \vdots \\
j & k \\
\vdots & \vdots \\
n_{J} & 1 \\
\vdots & \vdots \\
n_{J} & n_{K}
\end{bmatrix}
}\end{equation}

Additionally, we construct the matrix \(R\) to map each row of the
\(IA\) matrix with a corresponding row from the \(JK\) matrix. This
matrix has \(n\) rows and \(6\) columns, where
\(n = {n_{I} \cdot n_{A} \choose 2} \cdot n_{J} \cdot n_{K}\). Here, the
term \({n_{I} \cdot n_{A} \choose 2}\) represents the binomial
coefficient, which quantifies the total number of unique comparisons
possible between every pair of stimuli generated by each pair of
individuals in the population. Thus, we define the matrix as in Equation
(\ref{eq-mat12}). \begin{equation}\phantomsection\label{eq-mat12}{
R = \begin{bmatrix}
1 & 1 & 1 & 2 & 1 & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
1 & 1 & 1 & 2 & 1 & n_{K} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
i & a & h & b & j & k \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
n_{I} & n_{A}-1 & n_{I} & n_{A} & n_{J} & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
n_{I} & n_{A}-1 & n_{I} & n_{A} & n_{J} & n_{K}
\end{bmatrix}
}\end{equation}

It is easier to visualize the structure of the previously defined
vectors and matrices by considering an example. Assuming \(n_{I} = 5\),
\(n_{A} = 2\), \(n_{J} = 3\), and \(n_{K} = 3\), the vectors and
matrices described in Equations (\ref{eq-mat11}) and (\ref{eq-mat12})
take the form as in Equation (\ref{eq-mat13}).

\begin{equation}\phantomsection\label{eq-mat13}{
I = \begin{bmatrix}
1 \\
2 \\
3 \\
4 \\
5 
\end{bmatrix} ; \;
J = \begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix} ; \;
IA = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
2 & 1 \\
2 & 2 \\
3 & 1 \\
3 & 2 \\
4 & 1 \\
4 & 2 \\
5 & 1 \\
5 & 2 
\end{bmatrix} ; \;
JK = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
1 & 3 \\
2 & 1 \\
2 & 2 \\
2 & 3 \\
3 & 1 \\
3 & 2 \\
3 & 3 
\end{bmatrix} ; \;
R = \begin{bmatrix}
1 & 1 & 1 & 2 & 1 & 1 \\
1 & 1 & 1 & 2 & 1 & 2 \\
1 & 1 & 1 & 2 & 1 & 3 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
1 & 1 & 5 & 2 & 1 & 1 \\
1 & 1 & 5 & 2 & 1 & 2 \\
1 & 1 & 5 & 2 & 1 & 3 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
4 & 2 & 5 & 2 & 3 & 1 \\
4 & 2 & 5 & 2 & 3 & 2 \\
4 & 2 & 5 & 2 & 3 & 3 \\
5 & 1 & 5 & 2 & 3 & 1 \\
5 & 1 & 5 & 2 & 3 & 2 \\
5 & 1 & 5 & 2 & 3 & 3 
\end{bmatrix}
}\end{equation}

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{cp}_{R} & := f_{O}(D_{R}) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK})
\end{aligned}
\]

}

\subcaption{\label{fig-cj04_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.44\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_04.png}

}

\subcaption{\label{fig-cj04_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj04}Conceptual-population model, initial vectorized
form.}

\end{figure}%

Now, using Equations (\ref{eq-mat11}) and (\ref{eq-mat12}), we can
re-express SCM~\ref{fig-cj03_scm} and DAG~\ref{fig-cj03_dag} in an
equivalent vectorized form, as shown in Figure~\ref{fig-cj04}. In this
depiction, the outcome \(O^{cp}_{R}\), the texts' discriminal difference
\(D_{R}\), their discriminal processes \(T_{IA}\), and the judges'
biases \(B_{JK}\) are represented as vectors rather than scalar values.
These vectors capture all the observations from the conceptual
population. Specifically, \(O^{cp}_{R}\) and \(D_{R}\) are observed and
latent vectors of length \(n\), respectively. Moreover, \(T_{IA}\) and
\(B_{JK}\) are latent vectors of lengths \(n_{I} \cdot n_{A}\) and
\(n_{J} \cdot n_{K}\), respectively.

\subsubsection{Integrating hierarchical structural
components}\label{sec-theory-theoretical_P3}

Building on the principles of Structural Equation Modeling (SEM)
\citep{Hoyle_et_al_2023} and Item Response Theory (IRT)
\citep{Fox_2010, vanderLinden_et_al_2017_I}, the conceptual-population
model integrates two \emph{hierarchical structural components} to
examine how different \emph{relevant} \footnote{\emph{Relevant
  variables} are those that satisfy the \emph{backdoor criterion}
  \citep[pp 37]{Neal_2020}, that is, they belong to a \emph{sufficient
  adjustment set}
  \citep{Pearl_2009, Pearl_et_al_2016, Morgan_et_al_2014}. A
  \emph{sufficient} set (potentially empty) blocks all non-causal paths
  between a predictor and an outcome without opening new ones
  \citep{Pearl_2009}. Refer also to footnote 1.} variables--whether
observed or latent--affect the primary latent variable of interest
\citep{Everitt_et_al_2010}. This hierarchical design enables researchers
to formulate and test hypotheses that account for both the nested
structure of stimuli and the uncertainties inherent in trait estimation
(see Section~\ref{sec-theory-issue1b} and
Section~\ref{sec-theory-issue2} for a discussion of these
considerations).

The top branch of DAG~\ref{fig-cj09_dag} illustrates the first
component, where \emph{relevant} \footnote{refer to footnote 2.}
student-related variables \(X_{I}\), such as teaching method, and
students' idiosyncratic errors \(e_{I}\) causally influence the latent
variable representing students' writing-quality trait \(T_{I}\). The
error term \(e_{I}\) captures variations in students' traits unexplained
by \(X_{I}\). Here, \(X_{I}\) is an observed matrix with \(n_{I}\) rows
and \(q_{I}\) independent columns (variables), and both \(e_{I}\) and
\(T_{I}\) are latent vectors of length \(n_{I}\). Additionally, this
branch shows how \(T_{I}\), along with \emph{relevant} \footnote{refer
  to footnote 2.} text-related variables \(X_{IA}\) (e.g., text length),
and texts' idiosyncratic errors \(e_{IA}\) causally influence the texts'
written-quality trait \(T_{IA}\), the first primary latent variable of
interest. The error term \(e_{IA}\) captures variations in the texts'
traits that remain unexplained by \(T_{I}\) or \(X_{IA}\). Here,
\(X_{IA}\) is an observed matrix with dimensions \(n_{I} \cdot n_{A}\)
rows and \(q_{IA}\) independent columns (variables), while \(e_{IA}\)
and \(T_{IA}\) are latent matrices with \(n_{I}\) rows and \(n_{A}\)
columns.

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O^{cp}_{R} & := f_{O}(D_{R}) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\
  e_{I} & \:\bot\:\{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \:\bot\:\{ e_{IA}, e_{JK} \} \\
  e_{IA} & \:\bot\:e_{JK} 
\end{aligned}
\]

}

\subcaption{\label{fig-cj09_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_09.png}

}

\subcaption{\label{fig-cj09_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj09}Conceptual-population model, final vectorized
form.}

\end{figure}%

Similarly, the bottom branch of DAG~\ref{fig-cj09_dag} depicts the
second component, where \emph{relevant} \footnote{refer to footnote 2.}
judge-related variables \(Z_{J}\), such as judgment expertise, and
judges' idiosyncratic errors \(e_{J}\) causally influence the latent
variable representing judges' bias \(B_{J}\). The error \(e_{J}\)
captures variations in judges' bias unexplained by \(Z_{J}\). Here,
\(Z_{J}\) is an observed matrix with \(n_{J}\) rows and \(q_{J}\)
independent columns (variables), and both \(e_{J}\) and \(B_{J}\) are
latent vectors of length \(n_{J}\). Furthermore, the branch shows how
\(B_{J}\), along with \emph{relevant} \footnote{refer to footnote 2.}
judgment-related variables \(Z_{JK}\) (e.g., the number of judgments a
judge makes), and judgments' idiosyncratic errors \(e_{JK}\) causally
influence the judges' biases associated with each text \(B_{JK}\), the
second primary latent variable of interest. The error \(e_{JK}\)
captures variations in judgments unexplained by \(B_{J}\) or \(Z_{JK}\).
Here, \(Z_{JK}\) is an observed matrix with dimension
\(n_{J} \cdot n_{K}\) rows and \(q_{JK}\) independent columns
(variables), while \(e_{JK}\) and \(B_{JK}\) are latent latent matrices
with \(n_{J}\) rows and \(n_{K}\) columns

Notably, all variables and functions shown in SCM~\ref{fig-cj09_scm} and
DAG~\ref{fig-cj09_dag} are part of the set of endogenous variables
\(V\), structural equations \(F\), and exogenous variables \(E\) for the
conceptual-population model. Additionally, the figures demonstrate that
all exogenous variables are independent of one another, as indicated by
the relationships \(e_{IA} \:\bot\:\{ e_{I}, e_{JK}, e_{J} \}\),
\(e_{I} \:\bot\:\{ e_{JK}, e_{J} \}\) and \(e_{JK} \:\bot\:e_{J}\) and
the absence of connecting arrows.

Overall, the conceptual-population model extends Thurstone's general
form by introducing key innovations to address the limitations discussed
in Section~\ref{sec-theory-issue1b} and Section~\ref{sec-theory-issue2}.
These enhancements include accounting for judges' biases and integrating
hierarchical structural components. Nevertheless, despite its promise of
enhancing measurement accuracy and precision, the model still depends on
the unrealistic assumption that researchers have access to data from the
\emph{conceptual population}. Since researchers rarely meet this
assumption in practice, they must consider a more realistic scenario.

\subsection{The sample-comparison
model}\label{sec-theory-theoretical_SC}

The sample-comparison model presents a more realistic scenario than the
conceptual-population model. First, in
Section~\ref{sec-theory-theoretical_SC1}, it explicitly assumes
researchers work with a data sample consisting of a limited number of
repeated judgments \((n^{s}_{K})\) from a sample of judges
\((n^{s}_{J})\) and a specific number of texts \((n^{s}_{A})\) from a
sample of students \((n^{s}_{I})\), all drawn from the conceptual
population. Second, in Section~\ref{sec-theory-theoretical_SC2}, the
model assumes that judges do not perform \emph{all repeated judgments}
within the data sample. Instead, they conduct a sufficient number of
stimuli comparisons, \(n_{C}\), to ensure an accurate estimation of the
proportion \(P(B>A)\), as proposed by \citet{Thurstone_1927b}.

\subsubsection{The sample mechanism}\label{sec-theory-theoretical_SC1}

To incorporate the sampling mechanism and facilitate the interpretation
of the sample-comparison model, we first define the \emph{data sampling
process} using the binary vector variables \(S_{I}\), \(S_{J}\),
\(S_{IA}\), and \(S_{JK}\) as follows:
\begin{equation}\phantomsection\label{eq-mat21}{
S_{I} = \begin{bmatrix}
i_{(1)} \\
\vdots \\
i_{(i)} \\
\vdots \\
i_{(h)} \\
\vdots \\
i_{(nI)}
\end{bmatrix} ; \;
S_{J} = \begin{bmatrix}
j_{(1)} \\
\vdots \\
j_{(j)} \\
\vdots \\
j_{(nJ)}
\end{bmatrix} ; \;
S_{IA} = \begin{bmatrix}
ia_{(1,1)} \\
\vdots \\
ia_{(1,n_{A})} \\
\vdots \\
ia_{(i,a)} \\
\vdots \\
ia_{(h,b)} \\
\vdots \\
ia_{(nI,1)} \\
\vdots \\
ia_{(nI,nA)}
\end{bmatrix} ; \;
S_{JK} = \begin{bmatrix}
jk_{(1,1)} \\
\vdots \\
jk_{(1,n_{K})} \\
\vdots \\
jk_{(j,k)} \\
\vdots \\
jk_{(nJ,1)} \\
\vdots \\
jk_{(nJ,nK)}
\end{bmatrix}
}\end{equation}

Where each element of \(S_{I}\) is a binary value indicating the
presence or absence of corresponding elements in the vector \(I\), as in
Equation (\ref{eq-mat22}). We apply the same logic to \(S_{J}\) using
vector \(J\) (not shown). Thus, the vectors \(S_{I}\) and \(S_{J}\)
contains \(n_{I}\) and \(n_{J}\) elements, respectively.
\begin{equation}\phantomsection\label{eq-mat22}{
i_{(i)} = \begin{cases} 
1 & \text{if data element } i \text{ from } I \text{ is sampled} \\
0 & \text{if data element } i \text{ from } I \text{ is missing}
\end{cases}
}\end{equation}

Similarly, each element of \(S_{IA}\) is a binary value indicating the
presence or absence of data rows in the matrices \(IA\), as defined in
Equation (\ref{eq-mat23}). We apply the same logic to \(S_{JK}\) using
the matrix \(JK\) (not shown). Thus, the vectors \(S_{IA}\) and
\(S_{JK}\) contains \(n_{I} \cdot n_{A}\) and \(n_{J} \cdot n_{K}\)
elements, respectively. \begin{equation}\phantomsection\label{eq-mat23}{
ia_{(i,a)} = \begin{cases} 
1 & \text{if data elements } i,a \text{ from } IA \text{ are sampled} \\
0 & \text{if data elements } i,a \text{ from } IA \text{ are missing}
\end{cases}
}\end{equation}

We can illustrate the structure of these vectors more clearly with an
example. Suppose researchers exclude the second student, the second text
from each student, and the third judge from the setup shown in Equation
(\ref{eq-mat13}). Given \(n_{I} = 5\), \(n_{A} = 2\), \(n_{J} = 3\), and
\(n_{K} = 3\), the resulting vectors would have the following structure:

\begin{equation}\phantomsection\label{eq-mat24}{
S_{I} = \begin{bmatrix}
1 \\
0 \\
1 \\
1 \\
1
\end{bmatrix} ; \;
S_{J} = \begin{bmatrix}
1 \\
1 \\
0
\end{bmatrix} ; \;
S_{IA} = \begin{bmatrix}
1 \\
0 \\
0 \\
0 \\
1 \\
0 \\
1 \\
0 \\
1 \\
0 
\end{bmatrix} ; \;
S_{JK} = \begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
0 \\
0 \\
0 
\end{bmatrix}
}\end{equation}

Notably, Equation (\ref{eq-mat24}) shows that missing observations in
the vectors \(S_{I}\) and \(S_{J}\)--which represent unsampled students
and judges--directly determine which observations are missing in
\(S_{IA}\) and \(S_{JK}\). In other words, researchers can only observe
texts and judgments from students and judges initially included in the
sample. The equation also shows that the sum of observed elements in
\(S_{I}\) equals the number of sampled students \((n^{s}_{I})\) and that
a similar sum in vector \(S_{J}\) equals the sampled judges
\((n^{s}_{J})\). Conversely, the sum of observed elements in \(S_{IA}\)
represents the total sampled texts across all sampled students
\((n^{s}_{I} \cdot n^{s}_{A})\), while a similar sum in vector
\(S_{JK}\) represents the total sampled repeated judgments across all
sampled judges \((n^{s}_{J} \cdot n^{s}_{K})\). Notice that in this
example, because the design systematically excludes every third repeated
judgment, researchers can also express \(S_{JK}\) using
\(n_{K} = n^{s}_{K} = 2\).

Finally, we define the \emph{sample mechanism} \(S\) in Equation
(\ref{eq-mat25}), which maps each element of \(S_{IA}\) to every element
of \(S_{JK}\). Each element \(s_{(i,a,h,b,j,k)}\) is a binary value
indicating the presence or absence of data rows in the matrix \(R\)
resulting from the sample mechanism, as in Equation (\ref{eq-mat26}).
Thus, the vector contains \(n\) elements, matching the number of rows in
\(R\), and the sum of its elements represents the total data sample:
\(n^{s} = \binom{n^{s}_{I} \cdot n^{s}_{A}}{2} \cdot n^{s}_{J} \cdot n^{s}_{K}\).
Here, the term \({n^{s}_{I} \cdot n^{s}_{A} \choose 2}\) represents the
binomial coefficient, which quantifies the total number of unique
comparisons possible between every pair of sampled stimuli generated by
each pair of sampled individuals.

\begin{equation}\phantomsection\label{eq-mat26}{
s_{(i,a,h,b,j,k)} = \begin{cases} 
1 & \text{if data elements } i,a,h,b,j,k \text{ from } R \text{ are sampled} \\
0 & \text{if data elements } h,i,a,b,j,k \text{ from } R \text{ are missing}
\end{cases}
}\end{equation}

\begin{equation}\phantomsection\label{eq-mat25}{
S = \begin{bmatrix}
s_{(1,1,1,2,1,1)} \\
\vdots \\
s_{(1,1,1,2,1,n_{K})} \\
\vdots \\
s_{(i,a,h,b,j,k)} \\
\vdots \\
s_{(n_{I},n_{A}-1,n_{I},n_{A},n_{J},1)} \\
\vdots \\
s_{(n_{I},n_{A}-1,n_{I},n_{A},n_{J},1)}
\end{bmatrix}
}\end{equation}

With the definition of \(S\), we incorporate the sample mechanism into
the conceptual-population model. Following the convention of
\citet{McElreath_2020} and \citet{Deffner_et_al_2022},
DAG~\ref{fig-cj14_dag} represents the conceptual-population outcome
\(O^{cp}_{R}\) as unobserved, emphasizing that researchers cannot
directly access this outcome due to the sampling mechanism. The DAG also
depicts the \emph{sample design} vector \(S\) as a causal factor
influencing the sample-comparison outcome \(O^{sc}_{R}\). A square
encloses \(S\), indicating that it is a conditioned variable. In this
context, \emph{conditioning} means that researchers restrict their focus
to the elements of \(O^{cp}_{R}\) that satisfy \(s_{(i,a,h,b,j,k)}=1\)
\citep{Neal_2020, McElreath_2020}. In essence, \(S\) is a vector that
selects \emph{all repeated judgments made by a subset of judges for a
subset of stimuli produced by the sampled individuals}.

Notably, the DAG shows that \(S\) is independent of all other variables
in the model. This implies that DAG~\ref{fig-cj14_dag} applies
exclusively to Simple Random Sampling (SRSg) designs. In these designs,
each repeated judgment, judge, stimulus, and individual has the same
probability of being included in the sample as any other observation
within their respective groups \citep{Lawson_2015}.

However, due to concerns about the practical feasibility of the
comparison task {[}\citet{Boonen_et_al_2020}; {]}, CJ assessments rarely
implement an exhaustive pairings of sampled judges, stimuli, and
individuals. Thus, a realistic scenario must account for the fact that
judges typically compare only a subset of stimuli authored by a sample
of individuals.

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O_{R} & := f_{C}(O^{sc}_{R}, C) \\
  O^{sc}_{R} & := f_{S}(O^{cp}_{R}, S) \\
  O^{cp}_{R} & := f_{O}(D_{R}) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\
  e_{I} & \:\bot\:\{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \:\bot\:\{ e_{IA}, e_{JK} \} \\
  e_{IA} & \:\bot\:e_{JK} 
\end{aligned}
\]

}

\subcaption{\label{fig-cj14_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_14.png}

}

\subcaption{\label{fig-cj14_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj14}Sample-comparison model, final vectorized form}

\end{figure}%

\subsubsection{The comparison
mechanism}\label{sec-theory-theoretical_SC2}

As in the previous section, we begin defining the \emph{comparison
mechanism} using the binary vector variable \(C\) to facilitate the
interpretation of the sample-comparison model. Equation (\ref{eq-mat27})
shows that \(C\) contains \(n\) elements corresponding to the number of
rows in the \(R\) matrix, with each element \(c_{(i,a,h,b,j,k)}\) being
a binary value indicating the presence or absence of data rows in \(R\),
a definition similar to that of \(s_{(i,a,h,b,j,k)}\) in Equation
(\ref{eq-mat26}). \begin{equation}\phantomsection\label{eq-mat27}{
C = \begin{bmatrix}
c_{(1,1,1,2,1,1)} \\
\vdots \\
c_{(1,1,1,2,1,n_{K})} \\
\vdots \\
c_{(i,a,h,b,j,k)} \\
\vdots \\
c_{(n_{I},n_{A}-1,n_{I},n_{A},n_{J},1)} \\
\vdots \\
c_{(n_{I},n_{A}-1,n_{I},n_{A},n_{J},1)}
\end{bmatrix}
}\end{equation}

The DAG~\ref{fig-cj14_dag} also incorporates the \emph{comparison
mechanism} \(C\) into the conceptual-population model. It shows the
sample-comparison outcome \(O^{sc}_{R}\) as unobserved, emphasizing that
researchers cannot directly access this variable because of the
comparison mechanism. The DAG further shows \(C\) as a conditioned
variable (enclosed in a square) that causally influences the observed
outcome \(O_{R}\). This structure implies that \(C\) determines
\emph{which repeated judgments judges make for the stimuli produced by
the individuals}. In essence, \(C\) reflects the assumption that judges
\emph{do not} perform all possible repeated judgments but instead
complete a sufficient number, \(n_{C}\), to enable the accurate
estimation of the proportion \(P(B>A)\) for each stimulus pair
\citep[pp.~267]{Thurstone_1927b}.

Notably, DAG~\ref{fig-cj14_dag} also shows that \(C\) is independent of
all other variables in the model. This independence implies that the
conceptual model represented by the DAG applies exclusively to Random
Allocation Comparative Designs \citep{Bramley_2015}, or Incomplete Block
Designs \citep{Lawson_2015}, where every repeated judgment has an equal
probability of being included in the sample.

Finally, since it is standard to assume that the distribution of the
conceptual-population outcome \(O^{cp}_{R}\) also holds for
\(O^{sc}_{R}\) and \(O_{R}\), we can reformulate the sample-comparison
model in Figure~\ref{fig-cj14} into the equivalent form shown in
Figure~\ref{fig-cj15}. This reformulation produces a model that applies
directly to a sample of comparative data. In this version, the
unobserved outcomes \(O^{cp}_{R}\) and \(O^{sc}_{R}\) are omitted, and
\(O_{R}\) inherits the structural equation \(f_{O}\) that originally
defined \(O^{cp}_{R}\). Moreover, the definition of \(O_{R}\) now
reflects its direct dependence on the discriminal difference \(D_{R}\)
and the sample and comparison mechanisms, \(S\) and \(C\).

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\[
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\
  e_{I} & \:\bot\:\{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \:\bot\:\{ e_{IA}, e_{JK} \} \\
  e_{IA} & \:\bot\:e_{JK} 
\end{aligned}
\]

}

\subcaption{\label{fig-cj15_scm}SCM}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics[width=0.72\linewidth,height=\textheight,keepaspectratio]{./images/png/CJ_TM_15.png}

}

\subcaption{\label{fig-cj15_dag}DAG}

\end{minipage}%

\caption{\label{fig-cj15}Comparative judgment model}

\end{figure}%

In summary, the SCM~\ref{fig-cj15_scm} and DAG~\ref{fig-cj15_dag} extend
Thurstone's general form to address several limitations of the BTL
model. These extensions account for judge biases (see
Section~\ref{sec-theory-theoretical_P1}), reflect the hierarchical
structure of stimuli and incorporate measurement error in trait
estimation and hypothesis testing (see
Section~\ref{sec-theory-theoretical_P3}), and even clarify the role of
the sample and comparison mechanisms in CJ assessments (see
Section~\ref{sec-theory-theoretical_SC}). However, they do not resolve
concerns about the assumption of equal dispersions among stimuli
discussed in Section~\ref{sec-theory-issue1a}. Since this concern
relates to the statistical assumption underlying the distribution of the
discriminal process, we develop a formal statistical model to address it
in the next section.

\section{From SCM to statistical model}\label{sec-statistical}

Using the structural causal model (SCM) \ref{fig-cj15_scm}, we can
derive a statistical model that addresses violations of the equal
dispersion assumption (see Section~\ref{sec-theory-issue1a}). This
derivation is possible because a fully specified SCM encodes functional
and probabilistic information, which we can replace with suitable
functions and probabilistic assumptions \citep{Pearl_et_al_2016}.
Specifically, SCM~\ref{fig-cj15_scm} allows us to express the joint
distribution of our complex CJ system as a product of simpler
conditional probability distributions (CPDs)\footnote{This re-expression
  is possible because the \emph{chain rule} of probability and the
  \emph{Bayesian Network Factorization (BNF)} property. For further
  details, see \citet{Pearl_et_al_2016} and \citet{Neal_2020}.}, as
shown in Equation (\ref{eq-mat51}). For clarity, we treat expressions
such as \(Y := f_{Y}(X)\), \(P(Y \mid X)\), and \(Y \sim f(Y \mid X)\)
as equivalent, where \(P(Y \mid X)\) and \(f(Y \mid X)\) represent the
CPD of \(Y\) given \(X\).
\begin{equation}\phantomsection\label{eq-mat51}{
\begin{aligned}
  P(O_{R}, & S, C, D_{R}, T_{IA}, X_{IA}, e_{IA}, T_{I}, X_{I}, e_{I}, B_{JK}, Z_{JK}, e_{JK}, B_{J}, Z_{J}, e_{J} ) & \\
  &= P(O_{R} \mid D_{R}, S, C) \cdot P(S) \cdot P(C) \cdot P(D_{R} \mid T_{IA}, B_{JK}) \\
  & \quad \cdot P(T_{IA} \mid T_{I}, X_{IA}, e_{IA}) \cdot P(T_{I} \mid X_{I}, e_{I}) \\
  & \quad \cdot P(B_{JK} \mid B_{J}, Z_{JK}, e_{JK}) \cdot P(B_{J} \mid Z_{J}, e_{J}) \\
  & \quad \cdot P(X_{IA}) \cdot P(X_{I}) \cdot P(Z_{JK}) \cdot P(Z_{J}) \\
  & \quad \cdot P(e_{IA}) \cdot P(e_{I}) \cdot P(e_{JK}) \cdot P(e_{J})  
\end{aligned}
}\end{equation}

Each CPD in Equation (\ref{eq-mat51}) rests on specific assumptions,
which we outline in the statistical model \ref{fig-cj16_stat}. We begin
by assuming that \(O_{R}\) follows a Bernoulli distribution\footnote{The
  binomial distribution--including its special case, the Bernoulli
  distribution--represent a maximum entropy distribution for binary
  events \citep[pp.~34]{McElreath_2020}. This means that the Bernoulli
  distribution is the most consistent alternative when only two
  un-ordered outcomes are possible and their expected frequencies are
  assumed to be constant \citep[pp.~310]{McElreath_2020}. For a detailed
  discussion of the binomial as a maximum entropy distribution, see
  \citet[sec.~10.1.2]{McElreath_2020}.}, reflecting the binary nature of
CJ outcomes. Furthermore, following the conventions of Generalized
Linear Models (GLMs)
\citep{Nelder_et_al_1983, Nelder_et_al_1996, Agresti_2015}, the
distribution links \(O_{R}\) to the latent discriminal difference vector
\(D_{R}\) using an inverse-logit function:
\(\text{inv\_logit}(x) = 1/(1 + \exp(-x))\).

\begin{figure}

\begin{minipage}{0.33\linewidth}

\centering{

\[
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\ 
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\ \\
  e_{I} & \:\bot\:\{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \:\bot\:\{ e_{IA}, e_{JK} \} \\
  e_{IA} & \:\bot\:e_{JK}
\end{aligned}
\]

}

\subcaption{\label{fig-cj16_scm}SCM}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\[
\begin{aligned}
  & P( O_{R} \mid D_{R}, S, C ) \\
  & P( D_{R} \mid T_{IA}, B_{JK} ) \\
  & P( T_{IA} \mid T_{I}, X_{IA}, e_{IA} ) \\
  & P( T_{I} \mid X_{I}, e_{I} ) \\
  & P( B_{JK} \mid B_{J}, Z_{JK}, e_{JK} ) \\
  & P( B_{J} \mid Z_{J}, e_{J} ) \\ \\
  & P( e_{I} ) P( e_{IA} ) P( e_{J} ) P( e_{JK} ) \\ \\ \\
\end{aligned}
\]

}

\subcaption{\label{fig-cj16_prob}Probabilistic model}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\[
\begin{aligned}
  O_{R} & \overset{iid}{\sim} \text{Bernoulli} \left[ \text{inv\_logit}( D_{R} ) \right] \\
  D_{R} & = \left( T_{IA}[i,a] - T_{IA}[h,b] \right) + B_{JK}[j,k] \\
  T_{IA} & = T_{I} + \beta_{XA} X_{IA} + e_{IA} \\
  T_{I} & = \beta_{XI} X_{I} + e_{I} \\
  B_{JK} & = B_{J} + \beta_{ZK} Z_{JK} + e_{JK} \\
  B_{J} & = \beta_{ZJ} Z_{J} + e_{J} \\ \\
  \boldsymbol{e} & \sim \text{Multi-Normal}( \boldsymbol{\mu}, \boldsymbol{\Sigma} )
  \\
  \boldsymbol{\Sigma} &= \boldsymbol{V} \boldsymbol{Q} \boldsymbol{V} \\ \\
\end{aligned}
\]

}

\subcaption{\label{fig-cj16_stat}Statistical model}

\end{minipage}%

\caption{\label{fig-cj16}Comparative judgment model, SCM, probabilistic
and statistical model assuming different discriminal dispersions for the
student's traits}

\end{figure}%

Note that while the joint distribution in Equation (\ref{eq-mat51})
includes the probability distributions of the sampling and comparison
mechanisms, \(P(S)\) and \(P(C)\), as well as those of the predictor
variables--\(P(X_{IA})\), \(P(X_{I})\), \(P(Z_{JK})\), and
\(P(Z_{J})\)--we omit all of these probabilities from the statistical
model \ref{fig-cj16_stat}. This omission is justified because, while
these distributions contribute to the overall joint distribution of the
data, the variables \(S\), \(C\), \(X_{IA}\), \(X_{I}\), \(Z_{JK}\), and
\(Z_{J}\) are observed and independent of any other variable in the
model. As observed variables, they do not require distributional
assumptions in the same way the idiosyncratic errors do. Furthermore,
their independence follows from the underlying random selection
procedures that govern the variables\footnote{Randomization ensures that
  data--and, by extension, an estimator--satisfies several key
  identification properties, such as common support, no interference,
  and consistency. The most critical property, however, is the
  elimination of confounding. \emph{Confounding} occurs when an external
  variable, such as \(X_{I}\), simultaneously influences both the
  outcome (e.g., \(O_{R}\)) and a variable of interest (e.g., \(S\)),
  resulting in spurious associations between the latter two
  \citep{Everitt_et_al_2010}. Randomization ensure the absence of
  confounding by effectively decoupling the association between the
  variable of interest and any other variable, except for the outcome
  itself. For a more detailed discussion on the benefits of
  randomization, see \citet{Pearl_2009}, \citet{Morgan_et_al_2014},
  \citet{Neal_2020}, and \citet{Hernan_et_al_2025}.}.

Next we define \(D_{R}\) as the difference between the discriminal
processes \(T_{IA}[i, a]\) and \(T_{IA}[h, b]\), representing the
underlying written-quality trait of the compared texts, plus the
corresponding repeated judge bias \(B_{JK}[j, k]\). Note that if we
assume that \(B_{JK}[j,k]\) reflects the difference in stimulus-specific
biases, i.e., \(B_{JK}[j,k] = B_{JK}[i,a,j,k] - B_{JK}[h,b,j,k]\), we
can re-write the discriminal difference as:
\begin{equation}\phantomsection\label{eq-mat52}{
\begin{aligned}
D_{R} &= \left( T_{IA}[i,a] - T_{IA}[h,b] \right) + B_{JK}[j,k] \\
&= \left( T_{IA}[i, a] + B_{JK}[i, a, j, k] \right) - \left( T_{IA}[h, b] + B_{JK}[h, b, j, k] \right) \\
& = T^{*}_{IA}[i,a] - T^{*}_{IA}[h,b]
\end{aligned}
}\end{equation}

This formulation reveals that the discriminal difference captures a
\emph{pure interaction effect}, in which neither the texts' discriminal
processes nor the judges' biases alone determine the outcome, but their
interaction does \citep{Attia_et_al_2022}. Put simply, this mathematical
description captures the idea that the stimuli' discriminal processes
become an observable outcome only through the lens of judges'
perceptions (i.e., their biases). For clarity, the square brackets in
\(D_{R}\) indicate the relevant indices for each trait vector; they do
not imply any subsetting of the data.

We now specify the functional forms for \(T_{IA}\), \(T_{I}\),
\(B_{JK}\), and \(B_{J}\). We model \(T_{IA}\) as a linear combination
of the students' underlying writing-quality traits \(T_{I}\), the
effects of relevant text-related variables on quality assessment
\(\beta_{XA}X_{IA}\) (such as the influence of text length), and the
text-specific idiosyncratic errors \(e_{IA}\). Similarly, we express
\(T_{I}\) as a linear combination of relevant student-related variables
affecting the quality assessment \(\beta_{XI} X_{I}\), and
student-specific idiosyncratic errors \(e_{I}\). For the judge-specific
terms, we model \(B_{JK}\) as a linear combination of the judge's
individual bias \(B_{J}\), the influence of relevant judgment-related
variables on quality assessment \(\beta_{ZK}Z_{JK}\) (e.g., how the
number of judgments affect the evaluation), and judgment-specific
idiosyncratic errors \(e_{JK}\). Finally, we define \(B_{J}\) as a
linear combination of relevant judge-level variables influencing the
quality assessment \(\beta_{ZJ}Z_{J}\) (such as judgment expertise) and
judge-specific idiosyncratic errors \(e_{J}\).

Next, we specify the probabilistic assumptions for the idiosyncratic
errors \(e_{I}\), \(e_{IA}\), \(e_{J}\), and \(e_{JK}\). Unlike other
variables in the model, these error terms exhibit indeterminacies in
their \emph{location}, \emph{orientation}, and \emph{scale} due to the
lack of an inherent scale in the associated latent variables \(T_{I}\),
\(T_{IA}\), \(B_{J}\), and \(B_{JK}\). Thus, to identify the latent
variable model, we must resolve these indeterminacies
\citep{Depaoli_2021, deAyala_2009}. Drawing on principles from SEM
\citep{Hoyle_et_al_2023}, we assume that the vector of idiosyncratic
errors \(\boldsymbol{e} = [e_{I}, e_{IA}, e_{J}, e_{JK}]^{T}\), follows
a Multivariate Normal distribution with mean vector \(\boldsymbol{\mu}\)
and a covariance matrix
\(\boldsymbol{\Sigma} = \boldsymbol{V} \boldsymbol{Q} \boldsymbol{V}\),
with \(\boldsymbol{V}\) denoting a diagonal matrix of standard
deviations and \(\boldsymbol{Q}\) a correlation matrix. To address the
\emph{location} indeterminacy, we set the errors' mean vector to zero:
\begin{equation}\phantomsection\label{eq-mat53}{
\boldsymbol{\mu} = [0, 0, 0, 0]^{T}
}\end{equation}

Following SCM~\ref{fig-cj16_scm}, we resolve the \emph{orientation}
indeterminacy by assuming that the errors are uncorrelated. This
assumption leads us to define the error correlation matrix,
\(\boldsymbol{Q}\), as the identity matrix:
\begin{equation}\phantomsection\label{eq-mat54}{
\boldsymbol{Q} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 
  \end{bmatrix}
}\end{equation}

To resolve the \emph{scale} indeterminacy, we define the diagonal matrix
\(\boldsymbol{V}\) as: \begin{equation}\phantomsection\label{eq-mat55}{
\boldsymbol{V} = \begin{bmatrix}
    s_{XI} & 0 & 0 & 0 \\
    0 & p_{IA} & 0 & 0 \\
    0 & 0 & s_{ZJ} & 0 \\
    0 & 0 & 0 & p_{JK} 
  \end{bmatrix}
}\end{equation}

Here, \(s_{XI}\) represents the standard deviation for the individuals,
\(p_{IA}\) for the stimuli, \(s_{ZJ}\) for the judges, and \(p_{JK}\)
for the judgments. We assume \(s_{XI}\) varies depending on the teaching
method group to which each student belongs. Using the example from
Section~\ref{sec-theoretical}, where the teaching method
\(X_{I} = \{1,2\}\), the model sets the constraint according to Equation
(\ref{eq-mat56}). This constraint anchors the scale of the individuals'
latent trait while relaxing the assumption of equal dispersion for the
stimuli, thereby addressing the concerns raised in
Section~\ref{sec-theory-issue1a}.
\begin{equation}\phantomsection\label{eq-mat56}{
\sum_{g=1}^{2} s_{XI}[g]/2 = 1
}\end{equation}

Because the error vector \(\boldsymbol{e}\) follows an uncorrelated
Multivariate Normal distribution, the marginal distribution of
\(e_{IA}\) is a univariate Normal distribution with mean zero and
standard deviation \(p_{IA}\). We set \(P_{IA}\) as a proportion of
\(1\) to establish the scale of the stimuli' latent trait relative to
the scale of the individuals' trait. Note that as a result, \(T_{IA}\)
is also normally distributed. This configuration effectively reinstates
Thurstone's original assumption of Normal discriminal processes for the
stimuli (see Table~\ref{tbl-thurstone_cases}).

Similarly, we assume \(s_{ZJ}\) varies depending on the groups to which
each judge belongs. For instance, if \(Z_{J} = \{1,2,3\}\) represents
three groups of judges with varying expertise, the model sets the
constraint according to Equation (\ref{eq-mat57}). This constraint
anchors the scale of the judges' latent trait and relaxes the assumption
of equal dispersion for the judgments.
\begin{equation}\phantomsection\label{eq-mat57}{
\sum_{g=1}^{2} s_{ZJ}[g]/3 = 1
}\end{equation}

Conversely, \(p_{JK}\) is defined as a proportion of \(1\) to establish
the scale of the judgments' latent trait relative to the scale of the
judges' trait.

Finally, we use \emph{Bayesian inference methods} to convert the
statistical model \ref{fig-cj16_stat} into a practical statistical tool
for analyzing paired comparison data. Bayesian inference offers three
main advantages in this context. First, it handles complex and
overparameterized models, where the number of parameters exceeds the
number of observations \citep{Baker_1998, Kim_et_al_1999}. This feature
is essential for our implementation, as the proposed model is indeed
overparameterized. Second, it incorporates prior information to
constrain parameter estimates within plausible bounds, thereby
mitigating estimation issues like non-convergence or improper solutions
that often affect frequentist methods
\citep{Martin_et_al_1975, Seaman_et_al_2011}. We primarily use this
capability to define the error distribution and set the scale of latent
variables \citep{Depaoli_2014}. Third, Bayesian inference supports
robust inferences from small samples, where the asymptotic properties
underlying frequentist methods are less reliable
\citep{Baldwin_et_al_2013, Lambert_et_al_2006, Depaoli_2014}. This
feature is particularly relevant in CJ assessments, as researchers often
collect large volumes of paired comparisons but work with relatively
small samples of judges, stimuli, and individuals to test hypotheses.

The \textbf{Declarations} section of this document provides a link to
the model code, along with an alternative specification that assumes
equal discriminal dispersions. We tested both versions of the model with
success using \texttt{Stan} \citep[version 2.26.1]{Stan_2020}.

\section{Discussion}\label{sec-discussion}

Thurstone introduced the Law of Comparative Judgment to measure
psychological traits of stimuli through pairwise comparisons
\citep{Thurstone_1927a, Thurstone_1927b}. In its general form, the
theory models single-judge comparisons across multiple, potentially
correlated stimuli. Each comparison produces a dichotomous outcome
indicating which stimulus the judge perceives as having a higher trait
level. However, Thurstone identified one key challenge in this general
formulation: the measurement model required estimating more ``unknown''
parameters than the number of available pairwise comparisons
\citep{Thurstone_1927b}. To address this issue and facilitate the
theory's practical applicability, he formulated five cases, each
progressively incorporating several simplifying assumptions.

Among these, Case V remains the most widely used model in empirical CJ
research, mainly due to the widespread adoption of the BTL model. The
BTL model mirrors the core assumptions of Case V--namely, equal
discriminal dispersions and zero correlation among stimuli' discriminal
processes--but replaces the processes' normal distribution with the more
mathematically tractable logistic distribution
\citep{Andrich_1978, Bramley_2008}. Although this substitution has
minimal impact on trait estimation or model interpretation
\citep{vanderLinden_et_al_2017_I, McElreath_2021}, the simplifying
assumptions of the BTL model--and by extension, of Case V--may fail to
capture the complexity of some traits or account for heterogeneous
stimuli
\citep{Thurstone_1927a, Andrich_1978, Bramley_2008, Kelly_et_al_2022},
potentially leading to unreliable and inaccurate trait estimates
\citep{Ackerman_1989, Zimmerman_1994, McElreath_2020, Hoyle_et_al_2023}.

Moreover, because Thurstone's original goal was to produce a ``coarse
scaling'' of traits and allocate stimuli along this continuum
\citep[pp.~269]{Thurstone_1927a}, his theory offered no guidance on how
to use trait estimates for statistical inference. The CJ tradition has
attempted to address this gap by separating trait estimation from
hypothesis testing, relying on point estimates--such as BTL scores or
their transformations--for inference. Notably, while this approach
simplifies analysis, it can also introduce bias and compromise the
reliability of the resulting inferences
\citep{McElreath_2020, Kline_et_al_2023, Hoyle_et_al_2023}.

Thus, to address the limitations of Thurstone's Case V and the BTL
model, this study extends Thurstone's general form using a systematic,
integrated approach that combines causal and Bayesian inference methods.
The approach begins with the development of a conceptual model,
represented as a Structural Causal Model (SCM) and a Directed Acyclic
Graph (DAG)
\citep{Pearl_2009, Pearl_et_al_2016, Gross_et_al_2018, Neal_2020}. This
model integrates Thurstone's core theoretical principles, such as the
discriminal processes of stimuli, alongside key CJ assessment design
features, including judges' bias, sampling procedures, and comparison
mechanisms, thereby disentangling the causal processes underlying the CJ
system.

The approach then translates the SCM into a bespoke statistical model
that allows researchers to analyze CJ data when violations to the
assumptions of equal dispersion and zero correlation occur (see
Section~\ref{sec-theory-issue1a} and Section~\ref{sec-theory-issue1b})
and when statistical inference is necessary (refer to
Section~\ref{sec-theory-issue2}). In particular, this model accounts for
judge biases (see Section~\ref{sec-theory-theoretical_P1}), captures the
hierarchical structure of stimuli, incorporates measurement error into
the hypothesis testing process (refer to
Section~\ref{sec-theory-theoretical_P3}), and accommodates heterogeneity
in discriminal dispersions (see Section~\ref{sec-statistical}). By
addressing all these issues, these methodological innovations have the
potential to enhance the reliability and validity of trait measurement
in CJ \citep{Perron_et_al_2015}, while also improving the accuracy of
statistical inferences.

Beyond these potential benefits, the approach offers two additional
advantages. First, it clarifies the roles and interactions of all actors
and processes involved in CJ assessments. Second, it shifts the analytic
paradigm from passively accepting the assumptions of Case V and the BTL
model to actively testing their fit with observed data. Together, these
advantages establish a principled framework for evaluating best
practices in designing CJ assessments, one that better aligns with the
demands of contemporary CJ contexts \citep{Kelly_et_al_2022}, offering
new insights into existing research and opening promising avenues for
future inquiry.

\subsection{Future research directions using our
approach}\label{sec-discussion_RA}

Among the many potential directions for future research, three avenues
deserve particular attention due to their direct impact on the
reliability and validity of CJ trait estimates, as well as on the
accuracy of statistical inferences. The following sections outline these
avenues and explain how our approach facilitates their investigation.

\subsubsection{The impact of sampling and comparison
mechanisms}\label{sec-discussion_RA1}

Although sampling and comparison mechanisms are central to modern CJ
assessments, it is striking that most CJ literature has examined them
within a limited scope. Researchers have primarily investigated the
effects of adaptive comparative judgment (ACJ) designs on trait
reliability
\citep{Pollitt_2012a, Pollitt_2012b, Bramley_2015, Verhavert_et_al_2022, Mikhailiuk_et_al_2021, Gray_et_al_2024}
or proposed practical guidelines for the number of comparisons judges
should make \citep{Verhavert_et_al_2019, Crompvoets_et_al_2022}. While
these studies offer valuable insights, they also overlook the broader
role that these mechanisms play within the CJ system. As this oversight
likely stems from a more fundamental lack of conceptual clarity about
how these mechanisms function within the system, this study decided to
integrate these mechanisms into the conceptual model of CJ.

The explicit integration of the sampling and comparison mechanisms
offers a new perspective on how these mechanisms shape the CJ process.
Specifically, it clarifies their role as sources of missing data in CJ's
data-generating process--that is, as mechanisms that determine which
observations are missing from the final data sample. This new
perspective encourages the application of Little and Rubin's principled
missing data framework \citeyearpar{Little_et_al_2020}, allowing a more
rigorous evaluation of existing claims about these missing data
mechanisms, their influence on CJ outcomes, and their implications for
designing and evaluating more complex assessments setups.

Notably, this study circumvents the need to apply this missing data
framework by deliberately structuring the sampling and comparison
mechanisms to be independent of any observed or unobserved variables,
including the outcome. In other words, we designed these mechanisms so
they produce data that are \emph{missing completely at random} (MCAR)
\citep{Little_et_al_2020}. This design offers one key advantage: it
generates simple random samples that satisfy the condition of
\emph{ignorability}, allowing researchers to legitimately \emph{ignore}
missing data during analysis without introducing bias
\citep{Everitt_et_al_2010, Kohler_et_al_2019, Neal_2020}.

However, many modern CJ applications rely on more complex assessment
designs, in which the sampling and comparison mechanisms introduce more
intricate forms of missingness--such as \emph{missing at random} (MAR)
or \emph{missing not at random} (MNAR) \citep{Little_et_al_2020}. A
prominent example is the previously discussed ACJ designs, where prior
judgment outcomes inform the selection of stimulus pairs for subsequent
comparisons \citep{Pollitt_2012a, Pollitt_2012b, Bramley_2015}. This
pair selection process suggests that ACJ's comparison mechanism is
outcome-dependent, potentially classifying the method as a generator of
MNAR data. If this classification holds, the mixed findings on ACJ's
capabilities become more comprehensible: some studies find that the
method improves trait reliability
\citep{Pollitt_et_al_2003, Pollitt_2012a, Pollitt_2012b}, while others
contend that it artificially inflates these gains
\citep{Bramley_2015, Bramley_et_al_2019, Crompvoets_et_al_2020, Crompvoets_et_al_2022}.

Nevertheless, regardless of the underlying missingness mechanisms, any
CJ assessment design would benefit from explicitly defining its
assumptions--a practice supported by our approach. This clarity enables
researchers to evaluate how the sampling and comparison mechanisms
affect trait estimation and statistical inference within each design.
Such assessments are particularly relevant given the common
misconception in the CJ literature that Thurstone's model can naturally
handle even non-random missing data without compromising the reliability
or validity of trait estimates \citep{Bramley_2008}.

\subsubsection{The effects of judges' bias on the reliability of
traits}\label{sec-discussion_RA2}

Despite the growing notion that various stimulus-related factors
influence judges' perceptions
\citep{vanDaal_et_al_2016, Lesterhuis_et_al_2018, Chambers_et_al_2022}
and that these influences may not always cancel each other out, few
studies in the CJ literature provide empirical evidence for judges'
biases
\citep{Pollitt_et_al_2003, vanDaal_et_al_2016, Bartholomew_et_al_2020a}.
This gap likely persists not due to a lack of interest or research but
because researchers often rely on ad-hoc detection methods, such as
`misfit' statistics, that may not be well-suited for the task
\citep{Kelly_et_al_2022}. To address this limitation, this study treats
judges' biases as an integral component of the CJ system from the
outset. This approach offers one key advantage: it provides a more
accurate representation of the data-generating process behind pairwise
comparisons, one that acknowledges that the discriminal processes of
stimuli become an observable outcome only through judges' perceptions,
which may exhibit bias.

The explicit integration of judges' bias into CJ's conceptual model then
paves the way for investigating several relevant research questions. One
key question is whether researchers can validly analyze CJ data under
the assumption of ``sample-free'' trait calibration, specifically under
the hypothesis that judges exhibit no systematic bias. This question is
particularly relevant because many researchers still regard
``sample-freeness'' as an inherent property of the BTL model
\citep{Bramley_2008, Andrich_1978} despite growing evidence of
persistent biases. Another critical question, discussed in
Section~\ref{sec-theory-issue1b}, is whether training or expertise can
help judges avoid focusing on irrelevant stimulus features, such as
handwriting, over more central criteria like argumentative quality in
writing assessments \citep{Kelly_et_al_2022}. Notably, exploring these
questions may also provide insights into what it truly means to be an
``expert'' within the CJ context \citep{Kelly_et_al_2022}.

Moreover, since judges rely on these stimulus-related factors when
evaluating complex, multidimensional traits
\citep{vanDaal_et_al_2016, Lesterhuis_et_al_2018, Chambers_et_al_2022},
and these factors account for variation in judgment accuracy
\citep{Gill_et_al_2013, vanDaal_et_al_2017, vanDaal_2020, Gijsen_et_al_2021},
it is reasonable to expect that assessments also vary according to
judge-specific attributes such as gender, age, culture, income,
education, training, or expertise \citep{Kelly_et_al_2022}. Prior
studies support this view
\citep{Bartholomew_et_al_2020a, McMahon_et_al_2015}. Thus, building on
the discussion in Section~\ref{sec-discussion_RA1}, researchers could
further explore how judges' selection influences the formation of a
``shared consensus'' and whether these attributes introduce systematic
biases or distortions in the observed trait distribution
\citep{Deffner_et_al_2022}. Furthermore, if such attributes indeed
undermine the assumption of ``sample-freeness,'' it becomes essential to
explore strategies for mitigating their influence and to determine how
many judges (and how many judgments per judge) are needed to produce
reliable trait estimates under these conditions. In addition, it is
worth considering whether \emph{repeated measures designs}, in which
judges evaluate the same stimulus pairs multiple times
\citep{Lawson_2015}, can improve judgment consistency and accuracy. As
anticipated, our approach provides the necessary structure to
investigate rigorously these questions.

\subsubsection{The identification of `misfitting' judges and
stimuli}\label{sec-discussion_RA3}

Although the CJ literature clearly defines \emph{misfit} judges and
stimuli, CJ researchers have rarely examined how these observations
relate to Thurstonian theory. In particular, they have not identified
which elements of Thurstone's theory might account for the occurrence of
misfits. This disconnect likely stems from the fact that CJ researchers
derive misfit statistics from residual analysis and outlier detection
methods rather than from Thurstonian principles. Specifically,
\emph{misfit judges} are typically defined as those whose assessments
diverge significantly from the ``shared consensus''
\citep{Pollitt_2012a, Pollitt_2012b, vanDaal_et_al_2016, Goossens_et_al_2018, Wu_et_al_2022},
while \emph{misfit stimuli} are those that elicit more judgment
discrepancies than others
\citep{Pollitt_2004, Pollitt_2012a, Pollitt_2012b, Goossens_et_al_2018}.
Both definitions closely mirror the statistical concept of
outliers--that is, observations that deviate markedly from the rest of
the sample in which they occur \citep{Grubbs_1969}. But this resemblance
extends beyond the definitions themselves, as CJ researchers often
identify misfits using conventional outlier detection procedures, such
as transforming BTL model residuals into diagnostic statistics and
comparing them against predefined thresholds
\citep{Pollitt_2012a, Pollitt_2012b, Wu_et_al_2022}.

Nevertheless, it is not the classification of misfits as outliers that
raises concerns for trait measurement and inference, but rather the
prevailing CJ practice of identifying them through these ad-hoc
detection procedures and then excluding them from analysis, often
without providing empirical evidence for the various hypotheses proposed
to explain their occurrence in the first place. In this regard, it is
essential to recognize that outliers can only be defined relative to a
specific model \citep{McElreath_2020}. Thus, identification procedures
based on models like the BTL, which relies on strong assumptions, should
be approached with caution, as they may not be well-suited for the task
\citep{Kelly_et_al_2022}. Specifically, because the BTL model may not
always accurately reflect the underlying data-generating process of the
CJ system (see Section~\ref{sec-theory-issues}), detection procedures
grounded in this model risk compromising the accurate identification of
misfits. Moreover, the statistical literature cautions that excluding
outliers risks discarding valuable information \citep{Miller_2023} and
introducing bias into trait estimates. The direction and magnitude of
this bias are often unpredictable, as they depend on which observations
researchers exclude from the analysis
\citep{Zimmerman_1994, OHagan_2018, McElreath_2020}. Lastly, regarding
the hypotheses proposed to explain the presence of misfits, it is
evident that the BTL model lacks the flexibility needed to adequately
test many of them, largely due to its strong assumptions.

In contrast, our approach provides a rigorous framework that enables the
examination of several relevant hypotheses. For instance, researchers
could investigate whether misfit judges are those who exhibit (an
outlying degree of) systematic bias or greater variability in their
judgments compared to their peers. Similarly, researchers might explore
whether misfit stimuli exhibit more variable discriminal processes
relative to other stimuli or if they are genuinely outlying cases.
Moreover, since outliers are not necessarily ``bad data''
\citep{McElreath_2020}, our approach also offers a principled
alternative that retains misfits in the analysis without compromising
trait estimation or inference. This alternative involves adapting our
proposed models into robust measurement models \citep{McElreath_2020}--a
broad class of procedures designed to reduce the sensitivity of
parameter estimates to mild or moderate departures from model
assumptions \citep{Everitt_et_al_2010}. This alternative also relates to
a broader discussion in social science, which is: that when researchers
face low predictive capacity from their models, they should not only
search for ``new'' variables or alternative procedures but also consider
employing more sophisticated measurement models
\citep{Wainer_et_al_1978}.

\subsection{Study limitations and practical challenges for applied CJ
researchers}\label{sec-discussion_challenges}

The process of deriving conclusions from observed data always requires
assumptions, whether the data are observational or experimental
\citep{Kohler_et_al_2019, Deffner_et_al_2022}. Our approach is not an
exception to this fundamental principle. As with all approaches grounded
on causal inference, ours has the limitation of relying on expert
knowledge and assumptions about the variables' causal structure that are
often untestable at the outset \citep{Hernan_et_al_2025}. As such, the
approach does not seek to yield automatic answers when applied to a
given CJ dataset. Instead, it aims to encourage researchers to formulate
precise questions and to make their assumptions explicit, fostering a
generalizable understanding of the CJ system under study
\citep{Rohrer_et_al_2022, Deffner_et_al_2022, Sterner_et_al_2024}. This
clarity is critical because a model's ability to produce accurate
estimates and valid inferences depends heavily on how well the data and
inferential goals align with its underlying assumptions
\citep{Kohler_et_al_2019}. Although this alignment remains empirically
untested for the proposed models, the theory-driven nature of our
approach provides a solid foundation for future empirical evaluation of
its causal assumptions \citep{Deffner_et_al_2022}, which are well
supported by both theory and existing evidence (see
Section~\ref{sec-theory-issues} and Section~\ref{sec-theoretical}).

Moreover, this theoretical commitment to causal inference also
introduces several practical challenges that applied CJ researchers must
navigate. These fall into two main categories: first, acquiring the
foundational knowledge necessary to apply the approach effectively; and
second, dedicating greater attention to both conceptual and statistical
modeling.

\subsubsection{Required foundational
knowledge}\label{sec-discussion_challenges1}

To apply our approach effectively, CJ researchers must tackle two main
challenges related to the required foundational knowledge. First, they
need a solid understanding of causal inference principles. Second, they
must learn to translate the functional and probabilistic content of
conceptual models into bespoke statistical models. One example that
illustrates the importance of a sound knowledge of causal inference is
the recurrent assumption that predictor variables are ``relevant'' to
the research context--interpreting this relevance as their inclusion in
a \emph{sufficient adjustment set}
\citep{Pearl_2009, Pearl_et_al_2016, Morgan_et_al_2014}. However, we do
not fully explore what this assumption entails or its implications for
model specification, estimation, and inference. As a result, CJ
researchers must deeply engage with these and other complex ideas,
including how SCMs encode functional and probabilistic information. To
assist this effort, this study includes key references throughout the
study to guide motivated aaplied CJ researchers toward a deeper
understanding of these concepts \footnote{refer to footnote 1, 2, and
  the detailed online document referenced in the Declarations section}.

Developing the skills to translate conceptual models into bespoke
statistical models also presents challenges. Although Bayesian inference
methods offer a more accessible path for many applied CJ researchers to
develop these skills--by reducing the need for specialized knowledge in
areas like optimization theory, which frequentist methods often
require--they still demand a working knowledge of other technical
concepts such as probabilistic programming languages (PPLs), probability
distributions, and convergence. These requirements can be non-trivial to
master. To support CJ researchers in developing these skills, this study
provides a link to the statistical model code and alternative model
specifications in the \textbf{Declarations} section of this document
\footnote{Seminal texts on Bayesian inference methods, such as
  \citet{Gelman_et_al_2014} and \citet{McElreath_2020}, offer valuable
  support for developing a deeper understanding of these models.}.

\subsubsection{Attention to conceptual and statistical
modeling}\label{sec-discussion_challenges2}

Even after acquiring the necessary foundational knowledge, CJ
researchers still face two additional challenges when applying our
approach to their specific research context. First, they must verify
whether the conceptual model provides a \emph{faithful} \footnote{Avoid
  confusing this term with the \emph{faithfulness assumption}
  \citep{Neal_2020, Hernan_et_al_2025} described in the causal inference
  literature. For further details, see footnote 1.} representation of
the CJ system under study. Second, they need to assess whether the
statistical translation of the model can accurately estimate the
intended estimands (i.e., parameters) from empirical data. To ensure a
faithful representation of the CJ system, we encourage researchers to
treat our conceptual and statistical models as starting points rather
than universal solutions for all CJ designs or datasets. While these
models may prove adequate in some contexts, researchers cannot assume
this apriori. Instead, they need to adapt these models to their specific
contexts, paying close attention to assessment design features and
causal assumptions. As discussed in
Section~\ref{sec-discussion_challenges}, our approach facilitates this
process by offering a transparent framework for articulating new
assumptions and guiding the design of CJ assessments.

Conversely, to evaluate the estimation capabilities of the statistical
model, researchers must tackle the challenge of performing
identification analysis. As outlined in Section~\ref{sec-theoretical},
\emph{identification analysis} determines whether a statistical model
can accurately compute a given estimand (e.g., a parameter) based solely
on its (causal) assumptions, independent of random variability
\citep{Schuessler_et_al_2023}. Identification is crucial because it is a
necessary condition for consistency. \emph{Consistency} is the property
of an estimator (e.g., a statistical model) whose estimates converge to
the ``true'' value of an estimand as data size approaches infinity
\citep{Everitt_et_al_2010}. Without identification, consistency is
impossible--even with infinite, error-free data--and thus, meaningful
inference from finite samples cannot be achieved
\citep{Schuessler_et_al_2023}. Although performing identification
analysis through formal derivations may seem like a natural next step,
the complexity of the CJ system renders such an approach impractical at
the outset. Instead, simulation-based methods, such as power analysis,
provide a more practical and flexible alternative, enabling researchers
to examine the consistency of estimates without relying on cumbersome
mathematical proofs. Nonetheless, the development of formal
identification proofs remains a significant goal for future CJ research.
Importantly, our approach supports both strategies by providing the
probabilistic foundation for formal derivations and the necessary
statistical structure for simulation-based methods.

\newpage{}

\section*{Declarations}\label{declarations}
\addcontentsline{toc}{section}{Declarations}

\textbf{Funding:} The Research Fund (BOF) of the University of Antwerp
funded this project.

\textbf{Financial interests:} The authors declare no relevant financial
interests.

\textbf{Non-financial interests:} The authors declare no relevant
non-financial interests.

\textbf{Ethics approval:} The University of Antwerp Research Ethics
Committee confirmed that this study does not require ethical approval.

\textbf{Consent to participate:} Not applicable

\textbf{Consent for publication:} All authors have read and approved the
final version of the manuscript for publication.

\textbf{Data availability:} This study did not use any data.

\textbf{Materials and code availability:} The \texttt{CODE\ LINK}
section at the top of the digital document located at:
\url{https://jriveraespejo.github.io/paper2_manuscript/} provides access
to all materials and code.

\textbf{AI-assisted technologies in the writing process:} The authors
used various AI-based language tools to refine phrasing, optimize
wording, and enhance clarity and coherence throughout the manuscript.
They take full responsibility for the final content of the publication.

\textbf{CRediT authorship contribution statement:}
\emph{Conceptualization:} J.M.R.E, T.vD., S.DM., and S.G.;
\emph{Methodology:} J.M.R.E, T.vD., and S.DM.; \emph{Software:}
J.M.R.E.; \emph{Validation:} J.M.R.E.; \emph{Formal Analysis:} J.M.R.E.;
\emph{Investigation:} J.M.R.E; \emph{Resources:} T.vD. and S.DM.;
\emph{Data curation:} J.M.R.E.; \emph{Writing - original draft:}
J.M.R.E.; \emph{Writing - review and editing:} T.vD., S.DM., and S.G.;
\emph{Visualization:} J.M.R.E.; \emph{Supervision:} S.G. and S.DM.;
\emph{Project administration:} S.G. and S.DM.; \emph{Funding
acquisition:} S.G. and S.DM.

\newpage{}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\renewcommand{\bibsection}{}
\bibliography{references.bib}





\end{document}
